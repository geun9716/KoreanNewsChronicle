from selenium import webdriver
from bs4 import BeautifulSoup
import time
import requests
import re
import csv

class Big_Kinds_Crawler(object):
    def __init__(self):
        self.news_press = [] # 언론사
        self.news_headline = [] # 뉴스 헤드라인
        self.news_url = []  # 뉴스 url
        self.news_main_text = []    # 뉴스 본문
        self.news_date = []  # 뉴스 날짜

    @staticmethod
    def preprocess(text):
        text = re.sub('<.+?>|&nbsp;', '', str(text))    # <> 안 내용, &nbsp;(개행) 제거
        return text

    def crawl_news_url(self, date_start, date_end):   # 빅카인즈에서 뉴스별 url, 언론사, 헤드라인 크롤

        driver = webdriver.Chrome('/Users/manda/OneDrive/바탕 화면/Utilities/chromedriver_win32/chromedriver')
        driver.implicitly_wait(3)
        driver.get('https://www.bigkinds.or.kr/')

        driver.implicitly_wait(1)

        # 기간 설정
        driver.find_element_by_id('date-filter-btn').click()    # 기간 버튼
        #driver.find_element_by_css_selector('#date-filter-div > div > div:nth-child(1) > button:nth-child(1)').click() # 1일 버튼
        #driver.find_element_by_css_selector('#date-filter-div > div > div:nth-child(1) > button:nth-child(2)').click() # 1주 버튼
        #driver.find_element_by_css_selector('#date-filter-div > div > div:nth-child(1) > button:nth-child(3)').click() # 1개월 버튼
        #driver.find_element_by_css_selector('#date-filter-div > div > div:nth-child(1) > button:nth-child(4)').click() # 3개월 버튼
        #driver.find_element_by_css_selector('#date-filter-div > div > div:nth-child(1) > button:nth-child(5)').click() # 6개월 버튼
        #driver.find_element_by_css_selector('#date-filter-div > div > div:nth-child(1) > button:nth-child(6)').click() # 1년 버튼
        driver.find_element_by_id('search-begin-date').send_keys('\b\b\b\b\b\b\b\b\b\b' + date_start[0:4] + '-' + date_start[4:6] + '-' + date_start[6:]) # 시작 날짜 입력
        driver.find_element_by_id('search-end-date').send_keys('\b\b\b\b\b\b\b\b\b\b' + date_end[0:4] + '-' + date_end[4:6] + '-' + date_end[6:])   # 끝 날짜 입력
        driver.find_element_by_id('date-confirm-btn').click()   # 기간 적용 버튼

        # 언론사 설정
        driver.find_element_by_id('provider-filter-btn').click()    # 언론사 버튼  
        #driver.find_element_by_id('중앙지').click()    # 중앙지 체크박스
        #driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(1) > div > button:nth-child(1)').click()    # 경향
        driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(2)').click()    # 국민
        driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(3)').click()    # 내일
        driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(4)').click()    # 동아
        driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(5)').click()    # 문화
        driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(6)').click()    # 서울
        driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(7)').click()    # 세계
        #driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(8)').click()    # 조선
        driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(9)').click()    # 중앙
        driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(10)').click()   # 한겨레
        driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(11)').click()   # 한국

        driver.implicitly_wait(1)

        # 카테고리 설정
        driver.find_element_by_id('category-filter-btn').click()    # 카테고리 버튼
        driver.find_element_by_css_selector('#category-tree-wrap > ul > li:nth-child(1) > div > span:nth-child(3)').click() # 정치
        driver.find_element_by_css_selector('#category-tree-wrap > ul > li:nth-child(2) > div > span:nth-child(3)').click() # 경제
        driver.find_element_by_css_selector('#category-tree-wrap > ul > li:nth-child(3) > div > span:nth-child(3)').click() # 사회
        #driver.find_element_by_css_selector('#category-tree-wrap > ul > li:nth-child(4) > div > span:nth-child(3)').click() # 문화
        driver.find_element_by_css_selector('#category-tree-wrap > ul > li:nth-child(5) > div > span:nth-child(3)').click() # 국제
        #driver.find_element_by_css_selector('#category-tree-wrap > ul > li:nth-child(6) > div > span:nth-child(3)').click() # 지역
        #driver.find_element_by_css_selector('#category-tree-wrap > ul > li:nth-child(7) > div > span:nth-child(3)').click() # 스포츠
        #driver.find_element_by_css_selector('#category-tree-wrap > ul > li:nth-child(8) > div > span:nth-child(3)').click() # IT_과학

        driver.find_element_by_css_selector('#news-search-form > div > div > div > div.input-group.main-search__form > span > button').click() # 검색 버튼

        driver.implicitly_wait(3)

        driver.find_element_by_css_selector('#filter-tm-use').click()   #   인사, 부고, 동정, 포토 제외

        driver.implicitly_wait(1)

        driver.find_element_by_css_selector('#select1 > option:nth-child(3)').click()   # 과거순

        driver.implicitly_wait(1)

        driver.find_element_by_css_selector('#select2 > option:nth-child(4)').click()   # 100건씩 보기

        time. sleep(1)

        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')

        total_count = soup.select_one('#total-news-cnt').get_text().replace(',', '') # 날짜 범위 내 기사 개수
        page_count = (int(total_count) // 100) + 1 # 기사 개수 // 100건씩 보기 = 페이지 

        url = [] # 페이지 별 기사들의 url
        press = []  # 페이지 별 기사들의 언론사
        headline = []   #페이지 별 기사들의 헤드라인
        
        for page in range(1, page_count + 1):   # 1~마지막 페이지
            # 페이지 처리
            page_click = page % 7   # 페이지 버튼 7개
            if page_click != 1:
                if page_click == 0:
                    page_click = 7
                else:
                    page_click += 2
                driver.find_element_by_css_selector('#news-results-pagination > ul > li:nth-child(' + str(page_click) + ') > a').click()    # 페이지 클릭
            elif (page_click == 1) and (page > 1):
                driver.find_element_by_css_selector('#news-results-pagination > ul > li:nth-child(10) > a').click()  # 다음 목록으로 넘어가기 클릭
                
            time.sleep(1)
            
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')

            url = soup.select('#news-results > div > div > div > a')  # 기사 url 긁기
            press = soup.select('#news-results > div > div > div > a') # 언론사 긁기
            headline = soup.select('#news-results > div > div.news-item__body > h4') # 기사 제목 긁기
            date = soup.select('#news-results > div > div.news-item__body > div.news-item__meta > span.news-item__date')

            for w in range(0, len(url)):
                self.news_url.append(url[w].get('href'))
            for x in range(0, len(url)):
                self.news_press.append(press[x].get_text())
            for y in range(0, len(url)):
                self.news_headline.append(self.preprocess(headline[y].get_text).replace('\n', '').replace('\t', '').replace('>', '').strip())
            for z in range(0, len(url)):
                self.news_date.append(date[z].get_text().strip())
                
        driver.close()
    
    def crawl_news_text(self):  # 긁어온 url들 들어가서 뉴스 본문 긁기
        for url_count  in range(0, len(self.news_url)):
            url_html = requests.get(self.news_url[url_count]).content
            soup = BeautifulSoup(url_html, 'html.parser')

            # 각 기사의 언론사에 맞게 크롤
            #if '경향신문' in self.news_press[url_count]:
            #    self.news_main_text.append(self.preprocess(soup.select_one('#articleBody').get_text()))
            if '국민일보' in self.news_press[url_count]:
                self.news_main_text.append(self.preprocess(soup.select_one('#articleBody').get_text().replace('\n', ' ').replace('\t', ' ')))
            elif '내일신문' in self.news_press[url_count]:
                self.news_main_text.append(self.preprocess(soup.select_one('#contents > p').get_text().replace('\n', ' ').replace('\t', ' ')))
            elif '동아일보' in self.news_press[url_count]:
                self.news_main_text.append(self.preprocess(soup.select_one('#content > div > div.article_txt').get_text().replace('\n', ' ').replace('\t', ' ')))
            elif '문화일보' in self.news_press[url_count]:
                self.news_main_text.append(self.preprocess(soup.select_one('#NewsAdContent').get_text().replace('\n', ' ').replace('\t', ' ')))
            elif '서울신문' in self.news_press[url_count]:
                if 'go' in self.news_url[url_count]:
                    self.news_main_text.append(self.preprocess(soup.select_one('#article_content').get_text().replace('\n', ' ').replace('\t', ' ')))
                elif 'now' in self.news_url[url_count]:
                    self.news_main_text.append(self.preprocess(soup.select_one('#articleContent').get_text().replace('\n', ' ').replace('\t', ' ')))
                else:
                    self.news_main_text.append(self.preprocess(soup.select_one('#atic_txt1').get_text().replace('\n', ' ').replace('\t', ' ')))
            elif '세계일보' in self.news_press[url_count]:
                self.news_main_text.append(self.preprocess(soup.select_one('#article_txt > article').get_text().replace('\n', ' ').replace('\t', ' ')))
            #elif '조선일보' in self.news_press[url_count]:
            #    self.news_main_text.append(self.preprocess(soup.select_one('#articleBody').get_text()))
            elif '중앙일보' in self.news_press[url_count]:
                self.news_main_text.append(self.preprocess(soup.select_one('#article_body').get_text().replace('\n', ' ').replace('\t', ' ')))
            elif '한겨레' in self.news_press[url_count]:
                self.news_main_text.append(self.preprocess(soup.select_one('#a-left-scroll-in > div.article-text > div > div.text').get_text().replace('\n', ' ').replace('\t', ' ')))
            elif '한국일보' in self.news_press[url_count]:
                text = ""
                result_set = soup.select('body > div.wrap > div.container.end > div > div > div > p')
                for p in result_set:
                    text = text + p.get_text()
                self.news_main_text.append(self.preprocess(text))

    def save_data(self):    # 크롤한 정보드 CSV로 저장
        file = open('BigKinds.csv', 'w', encoding='utf-8', newline = '')
        writer = csv.writer(file)
        writer.writerow(["date", "headline", "url", "text"])
        for l in range(0, len(self.news_url)):
            writer.writerow([self.news_date[l], self.news_headline[l], self.news_url[l], self.news_main_text[l]])

        file.close()

if __name__ == "__main__":
    date_start = str(input('시작 날짜 입력 ex) 20200905 : '))
    date_end = str(input('끝 날짜 입력 ex) 20200909 : '))

    Crawler = Big_Kinds_Crawler()
    Crawler.crawl_news_url(date_start, date_end)
    Crawler.crawl_news_text()
    Crawler.save_data()

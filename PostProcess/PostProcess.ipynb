{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd000c48d4780e7fa7f4b695bd71b2ac9722a525fe5d5880a11d633cd1f8478baa2",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "00c48d4780e7fa7f4b695bd71b2ac9722a525fe5d5880a11d633cd1f8478baa2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "df = pd.read_csv(\"Data/2019_DBSCAN_new3.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='labels', ascending=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'].max()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## pmi 적용하지 않은 N_list에서 Counter를 적용해 상위 20개 추출 후 파일로 저장\n",
    "no_pmi=[]\n",
    "\n",
    "for i in range(df['labels'].max()+1):\n",
    "    #print(i,\": ====================\")\n",
    "    word_list=[]\n",
    "    for article in df[df['labels'] == i]['N_list']:\n",
    "        \n",
    "       \n",
    "        arr = re.sub(\"[\\['\\]]\",'', article).split(',')\n",
    "        for j in range(len(arr)):\n",
    "            arr[j]=arr[j].strip()\n",
    "    \n",
    "        word_list+=arr\n",
    "    \n",
    "    counts=Counter(word_list).most_common(20)\n",
    "    no_pmi.append(counts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('no_pmi_counter.txt','w',encoding='utf-8-sig') as f:\n",
    "    for i,item in enumerate(no_pmi):\n",
    "        f.write(\"{} cluster===========\\n\".format(i))\n",
    "        f.write(str(item))\n",
    "        f.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npmi(a, b, n, z):\n",
    "    x = math.log10((n*z)/(a*b))\n",
    "    y = math.log10(n/z)\n",
    "    return x / y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# 뉴스 기사별 PMI 체크\n",
    "merge_tokens = []\n",
    "docs = []\n",
    "for article in df['N_list']:\n",
    "    doc = []\n",
    "    com = []\n",
    "    context = []\n",
    "    merge_token = []\n",
    "    merge_token2 = []\n",
    "\n",
    "    # str -> list\n",
    "    for line in article.split('], ['):\n",
    "        arr = re.sub(\"[\\[,'\\]]\", '', line).split(' ')\n",
    "        doc.append(arr)\n",
    "        context += arr\n",
    "\n",
    "    #2 PMI 적용\n",
    "    for line in doc:\n",
    "        com += list(zip(line, line[1:]))\n",
    "    n = len(context)\n",
    "    cnt = Counter(context)\n",
    "    nmi = {}\n",
    "    z = Counter(com)\n",
    "    for pair in com:\n",
    "        pmi = npmi(cnt[pair[0]], cnt[pair[1]], n, z[pair])\n",
    "        if -1 < pmi < 1:\n",
    "            nmi[pair] = pmi\n",
    "    result = sorted(nmi.items(), key = lambda item : item[1], reverse=True)\n",
    "    merge_token = [key[0]+\"_\"+key[1] for key, item in result if item >= 0.91]\n",
    "    if len(merge_token) > 0:\n",
    "        merge_tokens+=merge_token\n",
    "    # 토큰을 합치는 내용\n",
    "    for token in merge_token:\n",
    "        for content in doc:\n",
    "            for i in range(len(content)-1):\n",
    "                cmp_str = '_'.join(content[i:i+2])\n",
    "                if cmp_str == token:\n",
    "                    del content[i:i+2]\n",
    "                    content.insert(i, token)\n",
    "    docs.append(doc)\n",
    "z = Counter(merge_tokens).most_common()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['N_list'] = docs\n",
    "df['tokens'] = merge_tokens\n",
    "df.to_csv(\"./Data/2019_DBSCAN.csv\", encoding=\"utf-8-sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# 클러스터별 pmi 체크\n",
    "cluster = {}\n",
    "for i in range(df['labels'].max()+1):\n",
    "    print(i,\": ================\")\n",
    "    word_list = []\n",
    "    com=[]\n",
    "    for article in df[df['labels'] == i]['N_list']:\n",
    "        # str -> list\n",
    "        for line in article.split('], ['):\n",
    "            arr = re.sub(\"[\\[,'\\]]\", '', line).split(' ')\n",
    "            word_list += arr\n",
    "            com += list(zip(arr, arr[1:]))\n",
    "    print(len(word_list))\n",
    "    print(len(com))\n",
    "    n = len(word_list)\n",
    "    cnt = Counter(word_list)\n",
    "    nmi = {}\n",
    "    z = Counter(com)\n",
    "    for pair in com:\n",
    "        pmi = npmi(cnt[pair[0]], cnt[pair[1]], n, z[pair])\n",
    "        nmi[pair] = pmi\n",
    "    result = sorted(nmi.items(), key = lambda item : item[1], reverse=True)\n",
    "    merge_token = [key[0]+\"_\"+key[1] for key, item in result if item >= 0.9]\n",
    "    # merge_token = [(key, item) for key, item in result if item >= 0.9]\n",
    "    print(merge_token)\n",
    "    cluster[i] = merge_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# 뉴스 기사별 N_GRAM 체크\n",
    "merge_tokens = []\n",
    "docs = []\n",
    "for article in df['N_list']:\n",
    "    doc = []\n",
    "    com = []\n",
    "    context = []\n",
    "    merge_token = []\n",
    "\n",
    "    # str -> list\n",
    "    for line in article.split('], ['):\n",
    "        arr = re.sub(\"[\\[,'\\]]\", '', line).split(' ')\n",
    "        doc.append(arr)\n",
    "        com += list(zip(arr, arr[1:]))\n",
    "\n",
    "    #2 N_GRAM 체크\n",
    "    z = Counter(com).most_common()\n",
    "    for item, value in z:\n",
    "        if value > 3:\n",
    "            merge_token.append(item[0]+'_'+item[1])\n",
    "    if len(merge_token) > 0:\n",
    "        merge_tokens += merge_token\n",
    "\n",
    "    # 토큰을 합치는 내용\n",
    "    for token in merge_token:\n",
    "        for content in doc:\n",
    "            for i in range(len(content)-1):\n",
    "                cmp_str = '_'.join(content[i:i+2])\n",
    "                if cmp_str == token:\n",
    "                    del content[i:i+2]\n",
    "                    content.insert(i, token)\n",
    "    docs.append(doc)\n",
    "z = Counter(merge_tokens).most_common()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#클러스터별 N GRAM\n",
    "\n",
    "\n",
    "for i in range(df['labels'].max()+1):\n",
    "    docs = []\n",
    "    merge_tokens = []\n",
    "    print(i, \"===================\")\n",
    "    for article in df[df['labels'] == i]['N_list']:\n",
    "        doc = []\n",
    "        com = []\n",
    "        context = []\n",
    "        merge_token = []\n",
    "\n",
    "        # str -> list\n",
    "        for line in article.split('], ['):\n",
    "            arr = re.sub(\"[\\[,'\\]]\", '', line).split(' ')\n",
    "            doc.append(arr)\n",
    "            com += list(zip(arr, arr[1:]))\n",
    "\n",
    "        #2 N_GRAM 체크\n",
    "        z = Counter(com).most_common()\n",
    "        for item, value in z:\n",
    "            if value > 3:\n",
    "                merge_token.append(item[0]+'_'+item[1])\n",
    "        if len(merge_token) > 0:\n",
    "            merge_tokens += merge_token\n",
    "\n",
    "        # 토큰을 합치는 내용\n",
    "        for token in merge_token:\n",
    "            for content in doc:\n",
    "                for i in range(len(content)-1):\n",
    "                    cmp_str = '_'.join(content[i:i+2])\n",
    "                    if cmp_str == token:\n",
    "                        del content[i:i+2]\n",
    "                        content.insert(i, token)\n",
    "        docs.append(doc)\n",
    "    z = Counter(merge_tokens).most_common()\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## pmi를 적용한 토큰 병합후, 이형동의어 사전에서 찾아 대체\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Data/K_231_CV_2019_NE_TFIDF.csv\", encoding='utf-8-sig')\n",
    "\n",
    "tokenlist={}\n",
    "\n",
    "with open(\"tokenlist.txt\",'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        token,word=line.split(',')\n",
    "        tokenlist[token]=word.strip()\n",
    "\n",
    "front=[]\n",
    "end=[]\n",
    "\n",
    "for key in tokenlist.keys():\n",
    "    first,second=key.split('_')\n",
    "    front.append(first)\n",
    "    end.append(second)\n",
    "\n",
    "docs=[]\n",
    "\n",
    "for article in df['N_list']:\n",
    "\n",
    "    doc=[]\n",
    "\n",
    "    for line in article.split('], ['):\n",
    "        arr = re.sub(\"[\\[,'\\]]\", '', line).split(' ')\n",
    "        del_nums=[]\n",
    "        new_arr=[]\n",
    "        for i in range(len(arr)-1):\n",
    "            if arr[i] in front:\n",
    "                if arr[i+1]==end[front.index(arr[i])]:\n",
    "                    #print(arr[i],arr[i+1])\n",
    "                    del_nums.append(i)\n",
    "                    del_nums.append(i+1)\n",
    "                    #arr.append(arr[i]+\" \"+arr[i+1])\n",
    "                    arr.append(tokenlist[arr[i]+\"_\"+arr[i+1]])\n",
    "\n",
    "      \n",
    "        for i in range(len(arr)):\n",
    "            if(i not in list(set(del_nums))):\n",
    "                new_arr.append(arr[i])   \n",
    "        \n",
    "        doc.append(new_arr)\n",
    "        \n",
    "        \n",
    "    docs.append(doc)\n",
    "\n",
    "#docs\n",
    "print(\"Finished\")\n",
    "    \n",
    "\n",
    "df2=df\n",
    "df2['N_list']=docs\n",
    "df2.to_csv(\"./Data/2019_kmeans_231.csv\", encoding=\"utf-8-sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "##pmi적용된 N_list에서 Couter를 이용해 상위 20개 토큰 추출 후 파일로 저장 및 토픽으로 저장\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"./Data/2019_kmeans_231.csv\", encoding='utf-8-sig')\n",
    "\n",
    "df_out=pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"./Data/2019_kmeans_231.csv\", encoding='utf-8-sig')\n",
    "\n",
    "df_out=pd.DataFrame()\n",
    "\n",
    "words=[]\n",
    "\n",
    "pmi_count=[]\n",
    "\n",
    "for i in range(df['labels'].max()+1):\n",
    "    word_list2=[]\n",
    "    #print(i,'-----------------')\n",
    "    for article in df[df['labels'] == i]['N_list']:\n",
    "        arr = re.sub(\"[\\['\\]]\", '', article).split(',')\n",
    "        for i in range(len(arr)):\n",
    "            arr[i]=arr[i].strip()\n",
    "        #for line in article.split('], ['):\n",
    "            #arr = re.sub(\"[\\[,'\\]]\", '', line).split(' ')\n",
    "        word_list2+=arr\n",
    "    count1=Counter(word_list2).most_common(20)\n",
    "    print(count1)\n",
    "    pmi_count.append(count1)\n",
    "    word = [key for key, item in count1]\n",
    "    words.append(word)\n",
    "\n",
    "with open('pmi_counter.txt','w',encoding='utf-8-sig') as f:\n",
    "    for i,item in enumerate(pmi_count):\n",
    "        f.write(\"{} cluster===========\\n\".format(i))\n",
    "        f.write(str(item))\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "df_out['topics']=words\n",
    "\n",
    "df_out.to_csv(\"./Data/2019_kmeans_topics_231.csv\", encoding=\"utf-8-sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##클러스터 N_list에서 Counter 적용후 상위 100개 토큰 추출\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "df1 = pd.read_csv(\"./Data/북미 정상 회담_token.csv\", encoding='utf-8-sig')\n",
    "df2 = pd.read_csv(\"./Data/강원도 산불_token.csv\", encoding='utf-8-sig')\n",
    "df3 = pd.read_csv(\"./Data/남북미 정상 회담_token.csv\", encoding='utf-8-sig')\n",
    "df4 = pd.read_csv(\"./Data/헝가리 유람선 사고_token.csv\", encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "dfs=[]\n",
    "dfs.append(df1)\n",
    "dfs.append(df2)\n",
    "dfs.append(df3)\n",
    "dfs.append(df4)\n",
    "\n",
    "whole=[]\n",
    "\n",
    "words_cluster=pd.DataFrame()\n",
    "\n",
    "for item in dfs:\n",
    "    words=[]\n",
    "    for line in item['N_list']:\n",
    "        arr=re.sub(\"[\\[,'\\]]\", '', line).split(' ')\n",
    "        words+=arr    \n",
    "    count=Counter(words).most_common(100)\n",
    "    word = [key for key, item in count]\n",
    "    whole.append(word)\n",
    "\n",
    "\n",
    "print(whole)\n",
    "words_cluster['topics']=whole\n",
    "words_cluster.to_csv(\"./Data/2019_cluster_topics2.csv\", encoding=\"utf-8-sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##클러스터 N_list에서 Counter 적영후 상위 20개를 추출\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df1 = pd.read_csv(\"./Data/북미 정상 회담_token.csv\", encoding='utf-8-sig')\n",
    "df2 = pd.read_csv(\"./Data/강원도 산불_token.csv\", encoding='utf-8-sig')\n",
    "df3 = pd.read_csv(\"./Data/남북미 정상 회담_token.csv\", encoding='utf-8-sig')\n",
    "df4 = pd.read_csv(\"./Data/헝가리 유람선 사고_token.csv\", encoding='utf-8-sig')\n",
    "\n",
    "df1=df1.sample(n=int(len(df1)*0.3))\n",
    "df2=df2.sample(n=int(len(df2)*0.3))\n",
    "df3=df3.sample(n=int(len(df3)*0.3))\n",
    "df4=df4.sample(n=int(len(df4)*0.3))\n",
    "\n",
    "\n",
    "\n",
    "dfs=[]\n",
    "dfs.append(df1)\n",
    "dfs.append(df2)\n",
    "dfs.append(df3)\n",
    "dfs.append(df4)\n",
    "\n",
    "whole=[]\n",
    "\n",
    "for item in dfs:\n",
    "    words=[]\n",
    "    for line in item['N_list']:\n",
    "        arr=re.sub(\"[\\[,'\\]]\", '', line).split(' ')\n",
    "        words+=arr    \n",
    "    count=Counter(words).most_common(20)\n",
    "    word = [key for key, item in count]\n",
    "    whole.append(word)\n",
    "\n",
    "\n",
    "print(whole)\n",
    "\n",
    "#words_cluster.to_csv(\"./Data/2019_cluster_topics.csv\", encoding=\"utf-8-sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BIGKINDS 기사 일부만 가지고와서 cosine 유사도의 평균 구하기\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "def get_topics(percent): \n",
    "\n",
    "    df1 = pd.read_csv(\"./Data/북미 정상 회담_token.csv\", encoding='utf-8-sig')\n",
    "    df2 = pd.read_csv(\"./Data/강원도 산불_token.csv\", encoding='utf-8-sig')\n",
    "    df3 = pd.read_csv(\"./Data/남북미 정상 회담_token.csv\", encoding='utf-8-sig')\n",
    "    df4 = pd.read_csv(\"./Data/헝가리 유람선 사고_token.csv\", encoding='utf-8-sig')\n",
    "\n",
    "    df1=df1.sample(n=int(len(df1)*percent))\n",
    "    df2=df2.sample(n=int(len(df2)*percent))\n",
    "    df3=df3.sample(n=int(len(df3)*percent))\n",
    "    df4=df4.sample(n=int(len(df4)*percent))\n",
    "\n",
    "    dfs=[]\n",
    "    dfs.append(df1)\n",
    "    dfs.append(df2)\n",
    "    dfs.append(df3)\n",
    "    dfs.append(df4)\n",
    "\n",
    "    whole=[]\n",
    "\n",
    "    for item in dfs:\n",
    "        words=[]\n",
    "        for line in item['N_list']:\n",
    "            arr=re.sub(\"[\\[,'\\]]\", '', line).split(' ')\n",
    "            words+=arr    \n",
    "        count=Counter(words).most_common(100)\n",
    "        word = [key for key, item in count]\n",
    "        whole.append(word)\n",
    "\n",
    "\n",
    "    return whole\n",
    "#print(whole)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#코사인 유사도 계산\n",
    "def cos_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    l2_norm = (np.sqrt(sum(np.square(v1))) * np.sqrt(sum(np.square(v2))))\n",
    "    similarity = dot_product / l2_norm     \n",
    "    \n",
    "    return similarity\n",
    "\n",
    "sims=[]\n",
    "for i in range(100):\n",
    "    doc_nouns_list=[]\n",
    "\n",
    "    cluster_topic = get_topics(1)[1]\n",
    "    oasis_topic= get_topics(0.1)[1]\n",
    "\n",
    "    doc_nouns_list.append(' '.join(cluster_topic))\n",
    "    doc_nouns_list.append(' '.join(oasis_topic))\n",
    "\n",
    "    #print(\"KNC_topic : \",doc_nouns_list[0])\n",
    "    #print(\"Oasis_topic : \",doc_nouns_list[1])\n",
    "\n",
    "    tfidf_vect_simple = TfidfVectorizer(min_df=1)\n",
    "    feature_vect_simple = tfidf_vect_simple.fit_transform(doc_nouns_list)\n",
    "\n",
    "    # TFidfVectorizer로 transform()한 결과는 Sparse Matrix이므로 Dense Matrix로 변환. \n",
    "    feature_vect_dense = feature_vect_simple.todense()\n",
    "\n",
    "    #첫번째 문장과 두번째 문장의 feature vector  추출\n",
    "    vect1 = np.array(feature_vect_dense[0]).reshape(-1,)\n",
    "    vect2 = np.array(feature_vect_dense[1]).reshape(-1,)\n",
    "\n",
    "    #첫번째 문장과 두번째 문장의 feature vector로 두개 문장의 Cosine 유사도 추출\n",
    "    similarity_simple = cos_similarity(vect1, vect2)\n",
    "\n",
    "    print('KNC_topic, Oasis_topic Cosine 유사도: {0:.3f}'.format(similarity_simple))\n",
    "    sims.append(similarity_simple)\n",
    "\n",
    "sims=np.array(sims)\n",
    "sims.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BIGKINDS와 INTIMES 토픽 하나 유사도 비교\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "doc_nouns_list=[]\n",
    "\n",
    "#코사인 유사도 계산\n",
    "def cos_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    l2_norm = (np.sqrt(sum(np.square(v1))) * np.sqrt(sum(np.square(v2))))\n",
    "    similarity = dot_product / l2_norm     \n",
    "    \n",
    "    return similarity\n",
    "\n",
    "    \n",
    "\n",
    "cluster_topic = ['헝가리', '사고', '유람선', '수색', '침몰', '한국인', '부다페스트', '시신', '강', '작업', '실종자', '구조', '인양', '확인', '현장', '정부', '선체', '현지', '이날', '수습']\n",
    "bigkinds_topic=['헝가리', '사고', '유람선', '침몰', '다뉴브강', '한국인', '부다페스트', '수색', '시신', '실종자', '인양', '구조', '현지', '확인', '현장', '정부', '한국', '작업', '수습', '선체']\n",
    "\n",
    "doc_nouns_list.append(' '.join(cluster_topic))\n",
    "doc_nouns_list.append(' '.join(bigkinds_topic))\n",
    "\n",
    "print(\"KNC_topic : \",doc_nouns_list[0])\n",
    "print(\"Oasis_topic : \",doc_nouns_list[1])\n",
    "\n",
    "tfidf_vect_simple = TfidfVectorizer(min_df=1,analyzer = 'c')\n",
    "feature_vect_simple = tfidf_vect_simple.fit_transform(doc_nouns_list)\n",
    "\n",
    "# TFidfVectorizer로 transform()한 결과는 Sparse Matrix이므로 Dense Matrix로 변환. \n",
    "feature_vect_dense = feature_vect_simple.todense()\n",
    "\n",
    "#첫번째 문장과 두번째 문장의 feature vector  추출\n",
    "vect1 = np.array(feature_vect_dense[0]).reshape(-1,)\n",
    "vect2 = np.array(feature_vect_dense[1]).reshape(-1,)\n",
    "\n",
    "#첫번째 문장과 두번째 문장의 feature vector로 두개 문장의 Cosine 유사도 추출\n",
    "similarity_simple = cos_similarity(vect1, vect2)\n",
    "\n",
    "print('KNC_topic, Oasis_topic Cosine 유사도: {0:.3f}'.format(similarity_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BIGKINDS와 INTIMES 전체 토픽 유사도 비교\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#코사인 유사도 계산\n",
    "def cos_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    l2_norm = (np.sqrt(sum(np.square(v1))) * np.sqrt(sum(np.square(v2))))\n",
    "    similarity = dot_product / l2_norm     \n",
    "    \n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"Data/2019_cluster_topics3.csv\", encoding='utf-8-sig')\n",
    "\n",
    "intime_topics=[]\n",
    "\n",
    "for line in df['topics']:\n",
    "    arr=re.sub(\"[\\[,'\\]]\", '', line).split(' ')\n",
    "    intime_topics.append(arr)\n",
    "\n",
    "sims=[]\n",
    "\n",
    "for i,item in enumerate(intime_topics):\n",
    "    cluster_topic = item\n",
    "    bigkinds_topic=['헝가리', '사고', '유람선', '침몰', '다뉴브강', '한국인', '부다페스트', '수색', '시신', '실종자', '인양', '구조', '현지', '확인', '현장', '정부', '한국', '작업', '수습', '선체']\n",
    "\n",
    "    doc_nouns_list=[]\n",
    "\n",
    "    doc_nouns_list.append(' '.join(cluster_topic))\n",
    "    doc_nouns_list.append(' '.join(bigkinds_topic))\n",
    "\n",
    "    #print(\"KNC_topic : \",doc_nouns_list[0])\n",
    "    #print(\"Oasis_topic : \",doc_nouns_list[1])\n",
    "\n",
    "    tfidf_vect_simple = TfidfVectorizer(min_df=1)\n",
    "    feature_vect_simple = tfidf_vect_simple.fit_transform(doc_nouns_list)\n",
    "\n",
    "    # TFidfVectorizer로 transform()한 결과는 Sparse Matrix이므로 Dense Matrix로 변환. \n",
    "    feature_vect_dense = feature_vect_simple.todense()\n",
    "\n",
    "    #첫번째 문장과 두번째 문장의 feature vector  추출\n",
    "    vect1 = np.array(feature_vect_dense[0]).reshape(-1,)\n",
    "    vect2 = np.array(feature_vect_dense[1]).reshape(-1,)\n",
    "\n",
    "    #첫번째 문장과 두번째 문장의 feature vector로 두개 문장의 Cosine 유사도 추출\n",
    "    similarity_simple = cos_similarity(vect1, vect2)\n",
    "\n",
    "    print('{0}: KNC_topic, Oasis_topic Cosine 유사도: {1:.3f}'.format(i,similarity_simple))\n",
    "    sims.append(similarity_simple)\n",
    "    if(similarity_simple>0.5):\n",
    "        print(\"KNC_topic : \",item)\n",
    "\n",
    "sims=np.array(sims)\n",
    "sims.mean()\n"
   ]
  }
 ]
}
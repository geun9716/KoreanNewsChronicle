{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "            date press  category  \\\n0     2019/02/01  국민일보  경제,사회,지역   \n1     2019/02/01  국민일보        정치   \n2     2019/02/01  국민일보        경제   \n3     2019/02/01  국민일보        정치   \n4     2019/02/01  국민일보  IT_과학,경제   \n...          ...   ...       ...   \n4981  2019/02/11  문화일보  사회,IT_과학   \n4982  2019/02/11  문화일보        경제   \n4983  2019/02/11  문화일보     지역,경제   \n4984  2019/02/11  중앙일보        경제   \n4985  2019/02/11  세계일보        경제   \n\n                                            headline  \\\n0                          상생 위한 대타협 ‘일자리 창출’ 새 길 열다   \n1                  외교 당국자 “2차 북·미 정상회담에서 평화체제 논의될 것”   \n2                          대우조선 민영화 방안은 현대중공업과 지주 합작   \n3     안보리 전문가 보고서 “韓, 정유 340t 신고 않고 대북 반입”… 제재 위반 지적   \n4                   안 터지네!… LG 스마트폰 매출 ‘뚝’ 영업손실률 19%   \n...                                              ...   \n4981                           IoT로 區民 이용시설 미세먼지 잡는다   \n4982                      차가운 故國… 독립유공자 후손 주거지원 26%뿐   \n4983                           구의역 일대 ‘첨단업무복합단지’ 잰걸음   \n4984              K5보다 더 많이 팔렸다···국민 중형차 넘보는 벤츠 E클래스   \n4985                       자동차 수리부품 65% 가격에 교환 가능해진다   \n\n                                                    url  \\\n0     http://news.kmib.co.kr/article/view.asp?arcid=...   \n1     http://news.kmib.co.kr/article/view.asp?arcid=...   \n2     http://news.kmib.co.kr/article/view.asp?arcid=...   \n3     http://news.kmib.co.kr/article/view.asp?arcid=...   \n4     http://news.kmib.co.kr/article/view.asp?arcid=...   \n...                                                 ...   \n4981  http://www.munhwa.com/news/view.html?no=201902...   \n4982  http://www.munhwa.com/news/view.html?no=201902...   \n4983  http://www.munhwa.com/news/view.html?no=201902...   \n4984      https://news.joins.com/article/olink/22952238   \n4985  http://www.segye.com/content/html/2019/02/11/2...   \n\n                                                   text  \n0     광주시와 현대차가 투자자로 참여하는 완성차 공장 설립이 ‘광주형 일자리’를 통해 험...  \n1     2월 말 2차 북·미 정상회담에서 한반도 평화체제를 위한 다자협상이 본격 논의될 것...  \n2     “양사(현대중공업과 대우조선해양)가 합병하는 게 아니고, ‘조선지주’ 밑에 수평적으...  \n3     한국 정부가 지난해 개성공단 내 남북공동연락사무소에 사용한 정유제품을 유엔 안전보장...  \n4     LG전자 스마트폰 사업이 좀처럼 반등 기회를 잡지 못하고 있다. 지난해 하반기 야심...  \n...                                                 ...  \n4981  영등포구 ‘스마트 행정’ 확대 주민센터 등 40곳 센서 달아담당자에 실시간 공기質 ...  \n4982  보훈처 ‘생활실태 현황·조사’ 기초수급자 많고 옥탑방 ‘전전’ 유족들 “복지불안 가...  \n4983  광진구 “올 하반기 조기 착공” 2023년까지 복합타운 10개동 빌딩·호텔·아파트·...  \n4984  기아자동차의 중형세단 K5가 더 많이 팔릴까, 메르세데스-벤츠의 중형세단 E클래스가...  \n4985  (왼쪽)싼타페TM 전방 좌측 펜더, (오른쪽)싼타페TM 전방 우측 펜더국토교통부(장...  \n\n[4986 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class Reader:\n",
    "    def __init__(self, filePath):\n",
    "        newses=[]\n",
    "        news=pd.read_csv(filePath,encoding=\"UTF-8\")\n",
    "        newses.append(news)\n",
    "        self.docs=pd.concat(newses,ignore_index=True)\n",
    "    \n",
    "    def read(self, id):\n",
    "        if id==0:\n",
    "            return 0\n",
    "        return str(self.docs['text'][id-1])\n",
    "\n",
    "reader = Reader('../Data/1_201902.csv')\n",
    "\n",
    "print(reader.docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['것', '수', '이', '나', '사람', '등', '우리', '때', '년', '말', '일', '때문', '그것', '일', '문제', '사회', '중', '씨', '지금', '속', '하나', '집', '월', '데', '자신', '내', '경우', '명', '생각', '시간', '그녀', '앞', '번', '여자', '개', '전', '사실', '점', '정도', '원', '소리']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "import re # 정규표현식 패키지\n",
    "from kiwipiepy import Kiwi, Option\n",
    "import tomotopy as tp\n",
    "\n",
    "file = open(\"한국어불용어100.txt\", 'r', encoding=\"utf-8\")\n",
    "stopword=[]\n",
    "\n",
    "while True:\n",
    "    line = file.readline()\n",
    "    if not line: break\n",
    "    wordlist = line.split('\\t')\n",
    "    if (wordlist[1].startswith('N')):\n",
    "        stopword.append(wordlist[0])\n",
    "\n",
    "\n",
    "kiwi=Kiwi()\n",
    "kiwi.extract_add_words(reader.read)\n",
    "kiwi.prepare()\n",
    "\n",
    "print(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopword)\n",
    "\n",
    "\n",
    "def tokenize(sent): # 파일의 라인을 분석할 tokenize 함수\n",
    "    res, score = kiwi.analyze(sent)[0] # 첫번째 결과를 사용한다, 분석할때 나오는 결과에서 단어만 추출\n",
    "    return [word\n",
    "            for word, tag, _, _ in res\n",
    "            if tag.startswith('N') or tag.startswith(\"VA\") and word not in stopwords] #불용어사전 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic #0\t경제, 미국, 기업, 세계, 상위, 년, 정치, 것, 트럼프, 부자\n",
      "Topic #0\t발굴, 유해, 년, 전사자, 아버지, 구, 전쟁, 월, 수, 만\n",
      "Topic #0\t당, 의원, 한국, 대표, 투쟁, 황, 것, 통합, 지도부, 국회\n",
      "Topic #0\t물가, 년, 상승, 전년, 대비, 가격, 등, 하락, 품목, 2019년\n",
      "Topic #0\t입당, 등, 일, 선거, 법, 연령, 고등학생, 예비, 대상, 당원\n",
      "Topic #0\t의원, 직, 없, 한국, 당, 총사퇴, 대표, 국민, 의장, 사퇴\n",
      "Topic #0\tP2P, 많, 자, 연체, 말, 기간, 계좌, 금융, 대출, 업체\n",
      "Topic #0\t대통령, 것, 국회, 법, 문, 야당, 국민, 군소, 편, 지지자\n",
      "Topic #0\t전셋값, 부동산, 수, 공급, 수요, 집값, 대책, 서울, 것, 정부\n",
      "Topic #0\t공무원, 국민, 사회, 경제, 규제, 공직, 새롭, 년, 성장, 정책\n",
      "Topic #0\t군사, 장관, 안보, 일, 군, 평화, 정, 미사일, 훈련, 태세\n",
      "Topic #0\t국회, 정치, 대한민국, 국민, 의장, 일, 년, 새롭, 문, 새해\n",
      "Topic #0\t공제, 신문, 소득, 년, 구독료, 혜택\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-1cc03e58a229>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#해당 경로의 파일을 받아와 한 라인씩 model에 추가\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLDAModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_cf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTermWeight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPMI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#추출하고 모델안의 문헌을 넣는다. 즉, 학습과정에 쓰일 문헌을 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#학습 초기화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-e6b21550c985>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(sent)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 파일의 라인을 분석할 tokenize 함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkiwi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# 첫번째 결과를 사용한다, 분석할때 나오는 결과에서 단어만 추출\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     return [word\n\u001b[0;32m      7\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tomotopy as tp\n",
    "\n",
    "\n",
    "#LDA 모델을 적용해서 토픽 추출, k는 topic 개수\n",
    "#alpha는 문헌-토픽 디리클레 분포의 하이퍼 파라미터\n",
    "#eta는 토픽-단어 디리클레 분포의 하이퍼 파라미터 두개다 상수인듯하다\n",
    "#min_cf는 단어의 최소 장서 빈도수, 전체 문헌내 출현빈도\n",
    "#min_df는 단어의 최서 문헌 빈도수, 출현한 문헌 숫자수 의미\n",
    "#tw는 용어 가중치 기법으로, ONE, IDF, PMI를 사용가능, ONE 보다는 PMI나 IDF 둘중 하나 사용 \n",
    "\n",
    "for i in range(len(reader.docs)): #해당 경로의 파일을 받아와 한 라인씩 model에 추가\n",
    "    model = tp.LDAModel(k=1, alpha=0.1, eta=0.01, min_cf=3,min_df=1, tw=tp.TermWeight.PMI)\n",
    "    model.add_doc(tokenize(reader.docs['text'][i])) #추출하고 모델안의 문헌을 넣는다. 즉, 학습과정에 쓰일 문헌을 생성\n",
    "\n",
    "    model.train(0) #학습 초기화\n",
    "\n",
    " \n",
    "    for i in range(0,100):\n",
    "        model.train(20) #문헌 학습, 안에 숫자는 깁스 샘플링의 반복횟수\n",
    "\t\t#이때 기본값으로 시스템내 가용한 모든 스레드의 개수사용, 그리고 병렬화 방법을 찾아서 실행시켜준다\n",
    "  \n",
    "    for i in range(model.k): #k는 토픽의 개수\n",
    "        res = model.get_topic_words(i, top_n=10) #하위 토픽 i에 해당하는 top_n개의 단어 반환\n",
    "        print('Topic #{}'.format(i), end='\\t')\n",
    "        print(', '.join(w for w, p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
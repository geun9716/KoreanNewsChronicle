{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "             date press category                             headline  \\\n0      2019/01/01   한겨레       정치     문희상 “임시정부 100년…새 역사 시작하는 새해 될 것”   \n1      2019/01/01  세계일보    지역,경제                  새해에도 암울한 대구·경북 지역경제   \n2      2019/01/01  세계일보    경제,지역            경남 승강기안전공단 지역과 상생 동반성장 활동   \n3      2019/01/01  중앙일보       정치   문 대통령 신년사 “한 분 한 분의 삶 나아지도록 노력하겠다”   \n4      2019/01/01  동아일보       경제       이마트, 올해 상반기 ‘한우 등심’ 초저가로 가격 동결   \n...           ...   ...      ...                                  ...   \n24467  2019/01/31  중앙일보    국제,정치        비핵화 협상 기로…뭉치는 북방3각, 멀어지는 남방3각   \n24468  2019/01/31  중앙일보    국제,정치          일본서 여성의원·활동가에 '강매 괴롭힘'…수법보니   \n24469  2019/01/31  국민일보    사회,지역    숨진 아내 몸에 ‘방어흔’… 이틀 만에 발견된 노부부의 비극   \n24470  2019/01/31  서울신문    국제,정치  “트럼프·시진핑, 북미정상회담 후 2월 말 정상회담 개최 논의”   \n24471  2019/01/31  내일신문       사회       자기주도학습 중심, 일산 지역 독학재수학원을 소개합니다   \n\n                                                     url  \\\n0      http://www.hani.co.kr/arti/politics/politics_g...   \n1      http://www.segye.com/content/html/2018/12/31/2...   \n2      http://www.segye.com/content/html/2018/12/31/2...   \n3           http://news.joins.com/article/olink/22844371   \n4        http://news.donga.com/3/all/20181231/93504466/3   \n...                                                  ...   \n24467      https://news.joins.com/article/olink/22934944   \n24468      https://news.joins.com/article/olink/22934950   \n24469  http://news.kmib.co.kr/article/view.asp?arcid=...   \n24470  http://www.seoul.co.kr/news/newsView.php?id=20...   \n24471      http://www.naeil.com/news_view/?id_art=303085   \n\n                                                    text  \n0      문희상 국회의장이 2019년 새해를 “대한민국 임시정부 100년, 임시의정원 100...  \n1      내년도 대구와 경북의 경제성장률이 1%대에 그칠 것이란 암울한 전망이 나왔다.31일...  \n2      경남 한국승강기안전공단이 지역과 상생하며 동반성장하는 다양한 활동에 나서고 있다.공...  \n3      문재인 대통령은 1일 “이 겨울, 더 따뜻하게 세상을 밝히라는 촛불의 마음 결코 잊...  \n4      이마트가 올해 상반기 동안 한우 등심 가격을 인하·동결한다고 1일 밝혔다. 이마트는...  \n...                                                  ...  \n24467  2014년 3월26일 네덜란드 헤이그 미국대사관. 버락 오바마 대통령이 박근혜 대통...  \n24468  일본에서 주장이 강한 여성 정치인이나 시민단체 활동가 등을 겨냥한 협박, 괴롭힘 사...  \n24469  기사 내용과 무관. 게티이미지뱅크전북 고창의 한 아파트에서 80대 노부부가 숨진 채...  \n24470  美언론 “류허, 中하이난서 정상회담 제안” 도널드 트럼프 미국 대통령과 시진핑 중국...  \n24471  정시 합격자 발표가 이어지면서 재수를 결심하는 수험생들이 적지 않다. 지금부터 10...  \n\n[24472 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class Reader:\n",
    "    def __init__(self, filePath):\n",
    "        newses=[]\n",
    "        news=pd.read_csv(filePath,encoding=\"UTF-8\")\n",
    "        newses.append(news)\n",
    "        self.docs=pd.concat(newses,ignore_index=True)\n",
    "    \n",
    "    def read(self, id):\n",
    "        if id==0:\n",
    "            return 0\n",
    "        return str(self.docs['text'][id-1])\n",
    "\n",
    "reader = Reader('./201901.csv')\n",
    "\n",
    "print(reader.docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['것', '수', '이', '나', '사람', '등', '우리', '때', '년', '말', '일', '때문', '그것', '일', '문제', '사회', '중', '씨', '지금', '속', '하나', '집', '월', '데', '자신', '내', '경우', '명', '생각', '시간', '그녀', '앞', '번', '여자', '개', '전', '사실', '점', '정도', '원', '소리']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "import re # 정규표현식 패키지\n",
    "from kiwipiepy import Kiwi, Option\n",
    "\n",
    "file = open(\"한국어불용어100.txt\", 'r', encoding=\"utf-8\")\n",
    "stopword=[]\n",
    "\n",
    "while True:\n",
    "    line = file.readline()\n",
    "    if not line: break\n",
    "    wordlist = line.split('\\t')\n",
    "    if (wordlist[1].startswith('N')):\n",
    "        stopword.append(wordlist[0])\n",
    "\n",
    "\n",
    "kiwi=Kiwi()\n",
    "kiwi.extract_add_words(reader.read)\n",
    "kiwi.prepare()\n",
    "\n",
    "print(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopword)\n",
    "\n",
    "def tokenize(sent): # 파일의 라인을 분석할 tokenize 함수\n",
    "    res, score = kiwi.analyze(sent)[0] # 첫번째 결과를 사용한다, 분석할때 나오는 결과에서 단어만 추출\n",
    "    return [word\n",
    "            for word, tag, _, _ in res\n",
    "            if tag.startswith('N') or tag.startswith(\"VA\") and word not in stopwords] #불용어사전 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic #0\t경제, 미국, 기업, 세계, 상위, 년, 정치, 것, 트럼프, 부자\n",
      "Topic #0\t발굴, 유해, 년, 전사자, 아버지, 구, 전쟁, 월, 수, 만\n",
      "Topic #0\t당, 의원, 한국, 대표, 투쟁, 황, 것, 통합, 지도부, 국회\n",
      "Topic #0\t물가, 년, 상승, 전년, 대비, 가격, 등, 하락, 품목, 2019년\n",
      "Topic #0\t입당, 등, 일, 선거, 법, 연령, 고등학생, 예비, 대상, 당원\n",
      "Topic #0\t의원, 직, 없, 한국, 당, 총사퇴, 대표, 국민, 의장, 사퇴\n",
      "Topic #0\tP2P, 많, 자, 연체, 말, 기간, 계좌, 금융, 대출, 업체\n",
      "Topic #0\t대통령, 것, 국회, 법, 문, 야당, 국민, 군소, 편, 지지자\n",
      "Topic #0\t전셋값, 부동산, 수, 공급, 수요, 집값, 대책, 서울, 것, 정부\n",
      "Topic #0\t공무원, 국민, 사회, 경제, 규제, 공직, 새롭, 년, 성장, 정책\n",
      "Topic #0\t군사, 장관, 안보, 일, 군, 평화, 정, 미사일, 훈련, 태세\n",
      "Topic #0\t국회, 정치, 대한민국, 국민, 의장, 일, 년, 새롭, 문, 새해\n",
      "Topic #0\t공제, 신문, 소득, 년, 구독료, 혜택\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-1cc03e58a229>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#해당 경로의 파일을 받아와 한 라인씩 model에 추가\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLDAModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_cf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTermWeight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPMI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#추출하고 모델안의 문헌을 넣는다. 즉, 학습과정에 쓰일 문헌을 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#학습 초기화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-e6b21550c985>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(sent)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 파일의 라인을 분석할 tokenize 함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkiwi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# 첫번째 결과를 사용한다, 분석할때 나오는 결과에서 단어만 추출\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     return [word\n\u001b[0;32m      7\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tomotopy as tp\n",
    "\n",
    "\n",
    "#LDA 모델을 적용해서 토픽 추출, k는 topic 개수\n",
    "#alpha는 문헌-토픽 디리클레 분포의 하이퍼 파라미터\n",
    "#eta는 토픽-단어 디리클레 분포의 하이퍼 파라미터 두개다 상수인듯하다\n",
    "#min_cf는 단어의 최소 장서 빈도수, 전체 문헌내 출현빈도\n",
    "#min_df는 단어의 최서 문헌 빈도수, 출현한 문헌 숫자수 의미\n",
    "#tw는 용어 가중치 기법으로, ONE, IDF, PMI를 사용가능, ONE 보다는 PMI나 IDF 둘중 하나 사용 \n",
    "\n",
    "for i in range(len(reader.docs)): #해당 경로의 파일을 받아와 한 라인씩 model에 추가\n",
    "    model = tp.LDAModel(k=1, alpha=0.1, eta=0.01, min_cf=3,min_df=1, tw=tp.TermWeight.PMI)\n",
    "    model.add_doc(tokenize(reader.docs['text'][i])) #추출하고 모델안의 문헌을 넣는다. 즉, 학습과정에 쓰일 문헌을 생성\n",
    "\n",
    "    model.train(0) #학습 초기화\n",
    "\n",
    " \n",
    "    for i in range(0,100):\n",
    "        model.train(20) #문헌 학습, 안에 숫자는 깁스 샘플링의 반복횟수\n",
    "\t\t#이때 기본값으로 시스템내 가용한 모든 스레드의 개수사용, 그리고 병렬화 방법을 찾아서 실행시켜준다\n",
    "  \n",
    "    for i in range(model.k): #k는 토픽의 개수\n",
    "        res = model.get_topic_words(i, top_n=10) #하위 토픽 i에 해당하는 top_n개의 단어 반환\n",
    "        print('Topic #{}'.format(i), end='\\t')\n",
    "        print(', '.join(w for w, p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "b9d3bf71fe82dc7159b853f8f5ac3e2334aa339c3d1653768411f33acf68962a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-8171591eedde>, line 1)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-8171591eedde>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pip install kiwipiepy #형태소 분석기\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install kiwipiepy #형태소 분석기\n",
    "pip install tomotopy #토픽 추출기\n",
    "\n",
    "pip install pyvis #이거는 아래 보고와서 깔도록, CTM에서 쓰임돠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['신년', '인터뷰', '사회', '평등', '연구', '대가', '로버트', '라이', '시', '캘리포니아', '대학', '교수', '중산', '소득', '결국', '밑', '지속', '가능', '경제', '성장', '기대', '수', '정부', '무엇', '계층', '이동', '사다리', '구실', '교육', '투자', '사회', '평등', '연구', '분야', '세계', '석학', '로버트', '라이', '시', '미국', '캘리포니아', '대학', '버클리', '캠퍼스', '공공정책', '대학원', '교수', '서울신문', '인터뷰', '양극', '해법', '공정', '교육', '상위', '부', '독점', '정치', '경제', '구조', '자신', '유리', '상황', '교육', '계층', '이동', '기회', '의미', '현', '미국', '사회', '라이', '시', '교수', '비판', '그간', '양극', '신음', '지구촌', '곳곳', '관심', '서울', '신문', '년', '인터뷰', '진행', '이유', '아래', '일문일답', '로버트', '라이', '시', '미국', '캘리포니아', '대', '공공정책', '대학원', '교수', '서울신문', '인터뷰', '세계', '경제', '중국', '미국', '식', '제로', '섬', '게임', '인식', '도널드', '트럼프', '미국', '대통령', '비판', '세계', '경제', '유기', '연결', '때문', '미국', '우선주의', '나라', '위협', '뿐', '결국', '미국', '자신', '위험', '로버트', '라이', '시', '교수', '제공', '세계', '경제', '양극', '경제', '양극', '사회', '상위', '계층', '부', '독점', '심화', '미국', '경우', '상위', '정치', '경제', '정책', '자신', '유리', '양극', '소외', '좌절', '정치', '부패', '도널드', '트럼프', '대통령', '분노', '소수자', '이민자', '이슬람', '교도', '분출', '미국', '사회', '분배', '시스템', '작동', '의미', '그간', '소득', '증가', '상위', '하위', '얼마', '년', '년', '초반', '하위', '가구', '상위', '기업', '이윤', '대부분', '근로자', '년', '중반', '상위', '년대', '소득', '증가', '전부', '상위', '독차지', '기업', '이윤', '근로자', '뜻', '기업', '이윤', '근로자', '임금', '측면', '터', '년', '기준', '때', '년', '미국', '기업', '순수', '생산', '이상', '증가', '근로자', '실질', '시급', '노동조합', '와해', '정부', '부자', '감', '기업', '감', '세', '추진', '최고경영자', '임금', '외', '규모', '인센티브', '정리', '근로자', '힘', '약화', '상위', '정치', '경제', '힘', '구도', '지금', '부', '편중', '부채', '귀족', '노조', '떼', '법', '노동조합', '역효과', '일부', '노조', '행동', '지적', '노조', '부', '집중', '견제', '장치', '년대', '년대', '미국', '기업', '곳', '중', '곳', '노조', '결성', '이때', '근로자', '이윤', '분위기', '노조', '중소기업', '효과', '년대', '후반', '노조', '약화', '기업', '이윤', '근로자', '경영진', '주주', '집중', '한국', '최저임금', '폭', '사회', '논란', '미국', '단계', '임금', '인상', '국가', '경제', '발전', '긍정', '효과', '점', '증명', '돈', '노동자', '소비', '일자리', '창출', '전반', '경제성장', '반대', '임금', '고용', '부정', '결과', '수', '점진', '임금', '인상', '필요', '트럼프', '행정부', '부자', '감', '세', '추진', '트럼프', '행정부', '부자', '감세', '법인세', '인하', '실제', '기업', '설비', '투자', '고용', '증가', '증거', '만', '달러', '억', '원', '감', '혜택', '제너럴모터스', '미시간', '오하이오', '공장', '노동자', '감원', '아마존', '스타벅스', '개', '미국', '기업', '2018년', '법인세', '푼', '일자리', '미지수', '그간', '부자', '증세', '공격', '방법', '제시', '이목', '부자', '증세', '효과', '미국', '역사', '차', '세계', '대전', '이후', '상황', '증명', '년', '년', '간', '상위', '소득', '계층', '고율', '세금', '당시', '미국', '중산층', '시기', '현재', '미국', '과언', '부자', '감세', '시작', '1980년대', '로널드', '레이건', '행정부', '내실', '경제성장', '경제', '양극', '시작', '년대', '중반', '빌', '클린턴', '행정부', '부자', '증세', '시행', '때', '경제', '상황', '한국', '대기업', '경제성장', '역할', '주장', '일반', '소수', '대기업', '경제', '집중', '현상', '장기', '경제', '성장', '정치', '위험', '미국', '경험', '기업', '독점', '지위', '혁신', '중소', '기업', '성장', '뿐', '대기업', '정치', '영향력', '행사', '민주', '퇴행', '결국', '지속', '가능', '경제성장', '화두', '듯', '정책', '수단', '정책', '단기간', '경제', '선', '순환', '구조', '수', '부자', '감세', '중산층', '소득', '감세', '건강', '직업', '교육', '투자', '장기', '경제', '성장', '중요', '투자', '교육', '부문', '유아기', '청소년기', '공교육', '강화', '계층', '이동', '사다리', '복원', '수', '교육', '분야', '성과', '시간', '소득', '중산층', '수', '장기', '시각', '정책', '연구', '지원', '낙수', '효과', '경제', '밑', '분수', '효과', '효과', '미국', '보호', '무역', '주의', '자국', '주의', '때문', '세계', '국가', '간', '양극', '수', '우려', '트럼프', '대통령', '세계', '경제', '편', '편', '제로', '섬', '게임', '인식', '중국', '경제', '미국', '생각', '세계', '경제', '유기', '연결', '통합', '시스템', '제품', '교환', '직접', '투자', '트럼프', '대통령', '무역', '전쟁', '미국', '우선주의', '나라', '번영', '복지', '위협', '뿐', '결국', '미국', '자신', '위협', '관세', '폭탄', '수단', '우려', '트럼프', '대통령', '위험', '무역', '전쟁', '결과', '중국', '경제', '하락세', '관세', '폭탄', '무역', '고립', '주의', '미국', '1930년대', '의원', '주도', '관세', '전쟁', '불황', '트럼프', '대통령', '무역', '전쟁', '결과', '역사', '한국', '어른', '새해', '덕담', '마디', '한국', '축복', '나라', '년', '경제', '번영', '정치', '민주', '세계', '모범', '북한', '관계', '원래', '남북', '하나', '워싱턴', '한준규', '특파원', 'hihi@seoul.co.kr로버', '라이', '시', '교수', '로버트', '라이', '시', '미', '캘리포니아', '대학', '버클리', '캠퍼스', '공공정책', '대학원', '교수', '사회', '평등', '연구', '세계', '진보', '정치', '경제', '학자', '빌', '클린턴', '행정부', '노동부', '장관', '당시', '타임', '지', '20세기', '최고', '각료', '인', '중', '선정', '버락', '오바마', '대통령', '경제', '자문', '위원', '활동', '저서', '미국', '나라', '경제학', '자본주의', '구하라', '대', '위기', '반복']\nDocument #0 has been loaded\n['이동현', '산업', '팀', '차장', '원자', '번호', '번', '원소', '니켈', '인류', '오랫동안', '사용', '금속', '미국', '센트', '동전', '니켈', '세기', '초', '미국', '유행', '싸구려', '영화관', '이름', '니켈로디언', '유래', '입장료', '센트', '극장', '때', '동전', '니켈', '니켈', '대부분', '철', '합금', '형태', '존재', '동전', '대부분', '구리', '니케', '합금', '백동', '성질', '빛깔', '오랫동안', '동전', '소재', '사용', '원소', '순수', '니켈', '존재', '거', '연금술', '덕분', '금', '꿈', '화학', '발전', '18세기', '스웨덴', '악', '크룬스', '테트', '남작', '순수', '니켈', '분리', '니켈', '명명', '가공', '자성', '장신구', '전자', '부품', '활용', '도', '니켈', '인체', '접촉', '때', '알러지', '독', '발현', '주의', '필요', '합금', '장신구', '두드러기', '니케', '때문', '년', '전', '정수기', '업체', '부품', '니켈', '성분', '나', '인체', '유해', '논란', '니켈', '납', '축', '전지', '이후', '차', '전지', '주요', '성분', '1990년대', '리튬전지', '전', '소형', '전자', '기기', '니켈', '카드뮴', '니케', '금속', '수소', '화물', '전지', '리튬', '이온', '전지', '한동안', '최근', '각광', '자동차', '2차전지', '니켈', '전지', '니켈', '함유량', '차', '전지', '니켈', '함유량', '주행', '거리', '수', '니켈', '코발트', '망간', '배터리', '대표', '최근', '수요', '니켈', '가격', '천정', '부지', '중', '현지', '시간', '영국', '런던', '금속', '거래소', '니케', '선물', '개월', '물', '가격', '만', '달러', '만', '원', '센트', '동전', '이름', '만큼', '싸구려', '금속', '니켈', '몸', '니케', 'ᆯ은', '제3세계', '윤리', '채굴', '분쟁', '광물', '반성', '욕망', '누구', '고통', '편의', '중요', '누구', '고통', '편의', '수', '노릇']\nTotal docs: 2\nTotal words: 0\nVocab size: 0\nIteration 0\tLL per word: nan\nIteration 1\tLL per word: nan\nTopic #0\t\nTopic #1\t\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "import re # 정규표현식 패키지\n",
    "from kiwipiepy import Kiwi, Option\n",
    "import tomotopy as tp\n",
    "\n",
    "filename=\"test.txt\"  #파일 경로\n",
    "kiwi=Kiwi()\n",
    "kiwi.prepare()\n",
    "stopwords = set([\"명\", \"어제\", \"등\", \"이\",\"하\",\"돠\",\"있\",\"일\",\"것\",\"들\",\"적\",\"그\",\"씨\",\"되\",\"없\",\"다\",\"이다\"])\n",
    "\n",
    "\n",
    "def tokenize(sent): # 파일의 라인을 분석할 tokenize 함수\n",
    "    res, score = kiwi.analyze(sent)[0] # 첫번째 결과를 사용한다, 분석할때 나오는 결과에서 단어만 추출\n",
    "    return [word\n",
    "            for word, tag, _, _ in res\n",
    "            if tag.startswith('N') and word not in stopwords] #불용어사전 적용\n",
    "\n",
    "#N으로 시작하는 NNG (일반명사), NNP(고유명사), NNB(의존명사) NR(수사), NP(대명사)가 있다\n",
    "\n",
    "#pat_tag = re.compile('NN[GP]')\n",
    "#def tokenizer(raw, user_data):\n",
    "#        res, _ = user_data()[0]\n",
    "#        for w, tag, start, l in res:\n",
    "#            if not pat_tag.match(tag) or len(w) <= 1: continue\n",
    "#            yield w + ('다' if tag.startswith('V') else ''), start, l\n",
    "\n",
    "#위의 코드와 같이 정규표현식을 통해 원하는 형태로 추출 가능 일단은 ,명사위주로 해놨다.\n",
    "            \n",
    "\n",
    "model = tp.LDAModel(k=2, alpha=0.1, eta=0.01, min_cf=20,min_df=10, tw=tp.TermWeight.PMI)\n",
    "#LDA 모델을 적용해서 토픽 추출, k는 topic 개수\n",
    "#alpha는 문헌-토픽 디리클레 분포의 하이퍼 파라미터\n",
    "#eta는 토픽-단어 디리클레 분포의 하이퍼 파라미터 두개다 상수인듯하다\n",
    "#min_cf는 단어의 최소 장서 빈도수, 전체 문헌내 출현빈도\n",
    "#min_df는 단어의 최서 문헌 빈도수, 출현한 문헌 숫자수 의미\n",
    "#tw는 용어 가중치 기법으로, ONE, IDF, PMI를 사용가능, ONE 보다는 PMI나 IDF 둘중 하나 사용 \n",
    "\n",
    "\n",
    "for i, line in enumerate(open(filename, encoding='utf-8')): #해당 경로의 파일을 받아와 한 라인씩 model에 추가\n",
    "    print(tokenize(line))\n",
    "    model.add_doc(tokenize(line)) #추출하고 모델안의 문헌을 넣는다. 즉, 학습과정에 쓰일 문헌을 생성\n",
    "    if i % 10 == 0: print('Document #{} has been loaded'.format(i))\n",
    " \n",
    "model.train(0) #학습 초기화\n",
    "\n",
    "print('Total docs:', len(model.docs)) #문헌들\n",
    "print('Total words:', model.num_words) #전체단어개수\n",
    "print('Vocab size:', model.num_vocabs) #전체 어휘개수\n",
    " \n",
    " \n",
    "for i in range(0,2):\n",
    "    print('Iteration {}\\tLL per word: {}'.format(i, model.ll_per_word))\n",
    "    model.train(1) #문헌 학습, 안에 숫자는 깁스 샘플링의 반복횟수\n",
    "\t\t#이때 기본값으로 시스템내 가용한 모든 스레드의 개수사용, 그리고 병렬화 방법을 찾아서 실행시켜준다\n",
    "  \n",
    "for i in range(model.k): #k는 토픽의 개수\n",
    "    res = model.get_topic_words(i, top_n=5) #하위 토픽 i에 해당하는 top_n개의 단어 반환\n",
    "    print('Topic #{}'.format(i), end='\\t')\n",
    "    print(', '.join(w for w, p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'nlptesting/document/datas.txt'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-08c6a34839ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# 이미 전처리된 코퍼스가 있으면 불러온다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'k.cps'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tomotopy\\utils.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'k.cps'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-08c6a34839ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     )\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# 입력 파일에는 한 줄에 문헌 하나씩이 들어가 있습니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkiwi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_analyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nlptesting/document/datas.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;31m# 전처리한 코퍼스를 저장한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'k.cps'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nlptesting/document/datas.txt'"
     ]
    }
   ],
   "source": [
    "import re # 정규표현식 패키지\n",
    "import tomotopy as tp # 토픽 모델링에 사용할 패키지\n",
    "from kiwipiepy import Kiwi # 한국어 형태소 분석에 사용할 패키지\n",
    "from pyvis.network import Network # 네트워크 시각화에 사용할 패키지\n",
    " \n",
    "try:\n",
    "    # 이미 전처리된 코퍼스가 있으면 불러온다.\n",
    "    corpus = tp.utils.Corpus.load('k.cps')\n",
    "except IOError:\n",
    "    # 없으면 전처리를 시작한다.\n",
    "    kiwi = Kiwi()\n",
    "    kiwi.prepare()\n",
    " \n",
    "    # 형태소 분석 후 사용할 태그, 원하는대로 넣을 수 있다.\n",
    "    \"\"\"pat_tag = re.compile('NN[GP]')\"\"\"\n",
    "    pat_tag = re.compile('NN[GP]|V[VA]|MAG|MM')\n",
    " \n",
    "    def tokenizer(raw, user_data):\n",
    "        res, _ = user_data()[0]\n",
    "        for w, tag, start, l in res:\n",
    "            if not pat_tag.match(tag) or len(w) <= 1: continue\n",
    "            yield w + ('다' if tag.startswith('V') else ''), start, l\n",
    " \n",
    "    corpus = tp.utils.Corpus(\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    # 입력 파일에는 한 줄에 문헌 하나씩이 들어가 있습니다.\n",
    "    corpus.process((line, kiwi.async_analyze(line)) for line in open('nlptesting/document/datas.txt', 'r',-1,'utf-8'))\n",
    "    # 전처리한 코퍼스를 저장한다.\n",
    "    corpus.save('k.cps')\n",
    " \n",
    "# 최소 10개 이상 문헌에 등장하고, 전체 출현빈도는 20 이상인 단어만 사용합니다.\n",
    "# 그리고 상위 10개 고빈도 단어는 분석에서 제외하구요\n",
    "# 주제 개수는 40개입니다.\n",
    "mdl = tp.CTModel(tw=tp.TermWeight.ONE, min_df=10, min_cf=20, k=10, corpus=corpus)\n",
    "mdl.train(0)\n",
    " \n",
    "# 문헌 수가 만 개 이상이라면 num_beta_sample을 1~5 정도로 줄여도 됨\n",
    "# 수 천개라면 최소 10 정도, 수 백개에 불과하다면 20 이상으로 키우는걸 권장\n",
    "mdl.num_beta_sample = 1\n",
    "print('Num docs:{}, Num Vocabs:{}, Total Words:{}'.format(\n",
    "    len(mdl.docs), len(mdl.used_vocabs), mdl.num_words\n",
    "))\n",
    "print('Removed Top words: ', *mdl.removed_top_words)\n",
    " \n",
    "# 총 1000회 깁스샘플링을 반복합니다.\n",
    "# 적절한 반복회수는 데이터에 따라 다릅니다.\n",
    "# ll_per_word 값의 증가가 둔화되거나 멈추는 지점까지만\n",
    "# 반복하는걸 추천합니다.\n",
    "for i in range(0, 1000, 20):\n",
    "    print('Iteration: {:04}, LL per word: {:.4}'.format(i, mdl.ll_per_word))\n",
    "    mdl.train(20)\n",
    "print('Iteration: {:04}, LL per word: {:.4}'.format(i, mdl.ll_per_word))\n",
    "\n",
    "for i in range(mdl.k):\n",
    "    res = mdl.get_topic_words(i, top_n=5)\n",
    "    print('Topic #{}'.format(i), end='\\t')\n",
    "    print(', '.join(w for w, p in res))\n",
    "\n",
    " # 학습된 결과를 시각화 합니다.\n",
    "g = Network(width=800, height=800, font_color=\"#333\")\n",
    "correl = mdl.get_correlations().reshape([-1])\n",
    "correl.sort()\n",
    " \n",
    "# 상관계수 상위 10%만 간선으로 잇습니다.\n",
    "top_tenth = mdl.k * (mdl.k - 1) // 10\n",
    "top_tenth = correl[-mdl.k - top_tenth]\n",
    " \n",
    "topic_counts = mdl.get_count_by_topics()\n",
    " \n",
    "for k in range(mdl.k):\n",
    "    label = \"#{}\".format(k)\n",
    "    title= ' '.join(word for word, _ in mdl.get_topic_words(k, top_n=8))\n",
    "    print('Topic', label, title)\n",
    "    label += '\\n' + ' '.join(word for word, _ in mdl.get_topic_words(k, top_n=3))\n",
    "    g.add_node(k, label=label, title=title, shape='ellipse', value=float(topic_counts[k]))\n",
    "    for l, correlation in zip(range(k - 1), mdl.get_correlations(k)):\n",
    "        if correlation < top_tenth: continue\n",
    "        g.add_edge(k, l, value=float(correlation), title='{:.02}'.format(correlation))\n",
    " \n",
    "g.barnes_hut(gravity=-1000, spring_length=20)\n",
    "g.show_buttons()\n",
    "# 시각화 파일이 topic_network.html 이라는 이름으로 저장됩니다.\n",
    "# 웹 브라우저로 열어서 확인해보세요.\n",
    "g.show(\"topic_network.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "b9d3bf71fe82dc7159b853f8f5ac3e2334aa339c3d1653768411f33acf68962a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kiwipiepy #형태소 분석기\n",
    "pip install tomotopy #토픽 추출기\n",
    "\n",
    "pip install pyvis #이거는 아래 보고와서 깔도록, CTM에서 쓰임돠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Document #0 has been loaded\n",
      "Document #10 has been loaded\n",
      "Document #20 has been loaded\n",
      "Document #30 has been loaded\n",
      "Document #40 has been loaded\n",
      "Document #50 has been loaded\n",
      "Document #60 has been loaded\n",
      "Document #70 has been loaded\n",
      "Document #80 has been loaded\n",
      "Document #90 has been loaded\n",
      "Document #100 has been loaded\n",
      "Document #110 has been loaded\n",
      "Document #120 has been loaded\n",
      "Document #130 has been loaded\n",
      "Document #140 has been loaded\n",
      "Document #150 has been loaded\n",
      "Document #160 has been loaded\n",
      "Document #170 has been loaded\n",
      "Document #180 has been loaded\n",
      "Document #190 has been loaded\n",
      "Document #200 has been loaded\n",
      "Document #210 has been loaded\n",
      "Document #220 has been loaded\n",
      "Document #230 has been loaded\n",
      "Document #240 has been loaded\n",
      "Document #250 has been loaded\n",
      "Document #260 has been loaded\n",
      "Document #270 has been loaded\n",
      "Document #280 has been loaded\n",
      "Document #290 has been loaded\n",
      "Document #300 has been loaded\n",
      "Document #310 has been loaded\n",
      "Document #320 has been loaded\n",
      "Document #330 has been loaded\n",
      "Document #340 has been loaded\n",
      "Document #350 has been loaded\n",
      "Document #360 has been loaded\n",
      "Document #370 has been loaded\n",
      "Document #380 has been loaded\n",
      "Document #390 has been loaded\n",
      "Document #400 has been loaded\n",
      "Document #410 has been loaded\n",
      "Document #420 has been loaded\n",
      "Document #430 has been loaded\n",
      "Document #440 has been loaded\n",
      "Document #450 has been loaded\n",
      "Document #460 has been loaded\n",
      "Document #470 has been loaded\n",
      "Document #480 has been loaded\n",
      "Document #490 has been loaded\n",
      "Document #500 has been loaded\n",
      "Document #510 has been loaded\n",
      "Document #520 has been loaded\n",
      "Document #530 has been loaded\n",
      "Document #540 has been loaded\n",
      "Document #550 has been loaded\n",
      "Document #560 has been loaded\n",
      "Document #570 has been loaded\n",
      "Document #580 has been loaded\n",
      "Document #590 has been loaded\n",
      "Document #600 has been loaded\n",
      "Document #610 has been loaded\n",
      "Document #620 has been loaded\n",
      "Document #630 has been loaded\n",
      "Document #640 has been loaded\n",
      "Document #650 has been loaded\n",
      "Document #660 has been loaded\n",
      "Document #670 has been loaded\n",
      "Document #680 has been loaded\n",
      "Document #690 has been loaded\n",
      "Document #700 has been loaded\n",
      "Document #710 has been loaded\n",
      "Document #720 has been loaded\n",
      "Document #730 has been loaded\n",
      "Document #740 has been loaded\n",
      "Document #750 has been loaded\n",
      "Document #760 has been loaded\n",
      "Document #770 has been loaded\n",
      "Document #780 has been loaded\n",
      "Document #790 has been loaded\n",
      "Document #800 has been loaded\n",
      "Document #810 has been loaded\n",
      "Document #820 has been loaded\n",
      "Document #830 has been loaded\n",
      "Document #840 has been loaded\n",
      "Document #850 has been loaded\n",
      "Document #860 has been loaded\n",
      "Document #870 has been loaded\n",
      "Document #880 has been loaded\n",
      "Document #890 has been loaded\n",
      "Document #900 has been loaded\n",
      "Document #910 has been loaded\n",
      "Document #920 has been loaded\n",
      "Document #930 has been loaded\n",
      "Document #940 has been loaded\n",
      "Document #950 has been loaded\n",
      "Document #960 has been loaded\n",
      "Document #970 has been loaded\n",
      "Document #980 has been loaded\n",
      "Document #990 has been loaded\n",
      "Document #1000 has been loaded\n",
      "Document #1010 has been loaded\n",
      "Document #1020 has been loaded\n",
      "Document #1030 has been loaded\n",
      "Document #1040 has been loaded\n",
      "Document #1050 has been loaded\n",
      "Document #1060 has been loaded\n",
      "Document #1070 has been loaded\n",
      "Document #1080 has been loaded\n",
      "Document #1090 has been loaded\n",
      "Document #1100 has been loaded\n",
      "Document #1110 has been loaded\n",
      "Document #1120 has been loaded\n",
      "Document #1130 has been loaded\n",
      "Document #1140 has been loaded\n",
      "Document #1150 has been loaded\n",
      "Document #1160 has been loaded\n",
      "Document #1170 has been loaded\n",
      "Document #1180 has been loaded\n",
      "Document #1190 has been loaded\n",
      "Document #1200 has been loaded\n",
      "Document #1210 has been loaded\n",
      "Document #1220 has been loaded\n",
      "Document #1230 has been loaded\n",
      "Document #1240 has been loaded\n",
      "Document #1250 has been loaded\n",
      "Document #1260 has been loaded\n",
      "Document #1270 has been loaded\n",
      "Document #1280 has been loaded\n",
      "Document #1290 has been loaded\n",
      "Document #1300 has been loaded\n",
      "Document #1310 has been loaded\n",
      "Document #1320 has been loaded\n",
      "Document #1330 has been loaded\n",
      "Document #1340 has been loaded\n",
      "Document #1350 has been loaded\n",
      "Document #1360 has been loaded\n",
      "Document #1370 has been loaded\n",
      "Document #1380 has been loaded\n",
      "Document #1390 has been loaded\n",
      "Document #1400 has been loaded\n",
      "Document #1410 has been loaded\n",
      "Document #1420 has been loaded\n",
      "Document #1430 has been loaded\n",
      "Document #1440 has been loaded\n",
      "Document #1450 has been loaded\n",
      "Document #1460 has been loaded\n",
      "Document #1470 has been loaded\n",
      "Document #1480 has been loaded\n",
      "Document #1490 has been loaded\n",
      "Document #1500 has been loaded\n",
      "Document #1510 has been loaded\n",
      "Document #1520 has been loaded\n",
      "Document #1530 has been loaded\n",
      "Document #1540 has been loaded\n",
      "Document #1550 has been loaded\n",
      "Document #1560 has been loaded\n",
      "Document #1570 has been loaded\n",
      "Document #1580 has been loaded\n",
      "Document #1590 has been loaded\n",
      "Document #1600 has been loaded\n",
      "Document #1610 has been loaded\n",
      "Document #1620 has been loaded\n",
      "Document #1630 has been loaded\n",
      "Document #1640 has been loaded\n",
      "Document #1650 has been loaded\n",
      "Document #1660 has been loaded\n",
      "Document #1670 has been loaded\n",
      "Document #1680 has been loaded\n",
      "Document #1690 has been loaded\n",
      "Document #1700 has been loaded\n",
      "Document #1710 has been loaded\n",
      "Document #1720 has been loaded\n",
      "Document #1730 has been loaded\n",
      "Document #1740 has been loaded\n",
      "Document #1750 has been loaded\n",
      "Document #1760 has been loaded\n",
      "Document #1770 has been loaded\n",
      "Document #1780 has been loaded\n",
      "Document #1790 has been loaded\n",
      "Document #1800 has been loaded\n",
      "Document #1810 has been loaded\n",
      "Document #1820 has been loaded\n",
      "Document #1830 has been loaded\n",
      "Document #1840 has been loaded\n",
      "Document #1850 has been loaded\n",
      "Document #1860 has been loaded\n",
      "Document #1870 has been loaded\n",
      "Document #1880 has been loaded\n",
      "Document #1890 has been loaded\n",
      "Document #1900 has been loaded\n",
      "Document #1910 has been loaded\n",
      "Document #1920 has been loaded\n",
      "Document #1930 has been loaded\n",
      "Document #1940 has been loaded\n",
      "Document #1950 has been loaded\n",
      "Document #1960 has been loaded\n",
      "Document #1970 has been loaded\n",
      "Document #1980 has been loaded\n",
      "Document #1990 has been loaded\n",
      "Document #2000 has been loaded\n",
      "Document #2010 has been loaded\n",
      "Document #2020 has been loaded\n",
      "Document #2030 has been loaded\n",
      "Document #2040 has been loaded\n",
      "Document #2050 has been loaded\n",
      "Document #2060 has been loaded\n",
      "Document #2070 has been loaded\n",
      "Document #2080 has been loaded\n",
      "Document #2090 has been loaded\n",
      "Document #2100 has been loaded\n",
      "Document #2110 has been loaded\n",
      "Document #2120 has been loaded\n",
      "Document #2130 has been loaded\n",
      "Document #2140 has been loaded\n",
      "Document #2150 has been loaded\n",
      "Document #2160 has been loaded\n",
      "Document #2170 has been loaded\n",
      "Document #2180 has been loaded\n",
      "Document #2190 has been loaded\n",
      "Document #2200 has been loaded\n",
      "Document #2210 has been loaded\n",
      "Document #2220 has been loaded\n",
      "Document #2230 has been loaded\n",
      "Document #2240 has been loaded\n",
      "Document #2250 has been loaded\n",
      "Document #2260 has been loaded\n",
      "Document #2270 has been loaded\n",
      "Document #2280 has been loaded\n",
      "Document #2290 has been loaded\n",
      "Document #2300 has been loaded\n",
      "Document #2310 has been loaded\n",
      "Document #2320 has been loaded\n",
      "Document #2330 has been loaded\n",
      "Document #2340 has been loaded\n",
      "Document #2350 has been loaded\n",
      "Document #2360 has been loaded\n",
      "Document #2370 has been loaded\n",
      "Document #2380 has been loaded\n",
      "Document #2390 has been loaded\n",
      "Document #2400 has been loaded\n",
      "Document #2410 has been loaded\n",
      "Document #2420 has been loaded\n",
      "Document #2430 has been loaded\n",
      "Document #2440 has been loaded\n",
      "Document #2450 has been loaded\n",
      "Document #2460 has been loaded\n",
      "Document #2470 has been loaded\n",
      "Document #2480 has been loaded\n",
      "Document #2490 has been loaded\n",
      "Document #2500 has been loaded\n",
      "Document #2510 has been loaded\n",
      "Document #2520 has been loaded\n",
      "Document #2530 has been loaded\n",
      "Document #2540 has been loaded\n",
      "Document #2550 has been loaded\n",
      "Document #2560 has been loaded\n",
      "Document #2570 has been loaded\n",
      "Document #2580 has been loaded\n",
      "Document #2590 has been loaded\n",
      "Document #2600 has been loaded\n",
      "Document #2610 has been loaded\n",
      "Document #2620 has been loaded\n",
      "Document #2630 has been loaded\n",
      "Document #2640 has been loaded\n",
      "Document #2650 has been loaded\n",
      "Document #2660 has been loaded\n",
      "Document #2670 has been loaded\n",
      "Document #2680 has been loaded\n",
      "Document #2690 has been loaded\n",
      "Document #2700 has been loaded\n",
      "Document #2710 has been loaded\n",
      "Document #2720 has been loaded\n",
      "Document #2730 has been loaded\n",
      "Document #2740 has been loaded\n",
      "Document #2750 has been loaded\n",
      "Document #2760 has been loaded\n",
      "Document #2770 has been loaded\n",
      "Document #2780 has been loaded\n",
      "Document #2790 has been loaded\n",
      "Document #2800 has been loaded\n",
      "Document #2810 has been loaded\n",
      "Document #2820 has been loaded\n",
      "Document #2830 has been loaded\n",
      "Document #2840 has been loaded\n",
      "Document #2850 has been loaded\n",
      "Document #2860 has been loaded\n",
      "Document #2870 has been loaded\n",
      "Document #2880 has been loaded\n",
      "Document #2890 has been loaded\n",
      "Document #2900 has been loaded\n",
      "Document #2910 has been loaded\n",
      "Document #2920 has been loaded\n",
      "Total docs: 703\n",
      "Total words: 208293\n",
      "Vocab size: 2137\n",
      "Iteration 0\tLL per word: -20.047415135867034\n",
      "Iteration 20\tLL per word: -15.38143849139445\n",
      "Iteration 40\tLL per word: -15.181291826391677\n",
      "Iteration 60\tLL per word: -15.078118371453739\n",
      "Iteration 80\tLL per word: -15.008486044383826\n",
      "Iteration 100\tLL per word: -14.94543090489426\n",
      "Iteration 120\tLL per word: -14.9114822178482\n",
      "Iteration 140\tLL per word: -14.889483266418512\n",
      "Iteration 160\tLL per word: -14.869423840223037\n",
      "Iteration 180\tLL per word: -14.849526436516504\n",
      "Iteration 200\tLL per word: -14.846627052208431\n",
      "Iteration 220\tLL per word: -14.837268272367506\n",
      "Iteration 240\tLL per word: -14.827962825337261\n",
      "Iteration 260\tLL per word: -14.82482324358254\n",
      "Iteration 280\tLL per word: -14.817971551387057\n",
      "Iteration 300\tLL per word: -14.81290164196086\n",
      "Iteration 320\tLL per word: -14.816031877376883\n",
      "Iteration 340\tLL per word: -14.804416784352833\n",
      "Iteration 360\tLL per word: -14.803714409249858\n",
      "Iteration 380\tLL per word: -14.802678811912301\n",
      "Iteration 400\tLL per word: -14.800495137297808\n",
      "Iteration 420\tLL per word: -14.790806806144497\n",
      "Iteration 440\tLL per word: -14.789880044818807\n",
      "Iteration 460\tLL per word: -14.784240702109269\n",
      "Iteration 480\tLL per word: -14.789402420472262\n",
      "Iteration 500\tLL per word: -14.78635981907474\n",
      "Iteration 520\tLL per word: -14.787936454154526\n",
      "Iteration 540\tLL per word: -14.782662130649292\n",
      "Iteration 560\tLL per word: -14.779031688758801\n",
      "Iteration 580\tLL per word: -14.776451292780623\n",
      "Iteration 600\tLL per word: -14.772204952538155\n",
      "Iteration 620\tLL per word: -14.770263018609318\n",
      "Iteration 640\tLL per word: -14.767811276715092\n",
      "Iteration 660\tLL per word: -14.76679138917888\n",
      "Iteration 680\tLL per word: -14.769558706604489\n",
      "Iteration 700\tLL per word: -14.773280022661897\n",
      "Iteration 720\tLL per word: -14.771618453484354\n",
      "Iteration 740\tLL per word: -14.771407394123697\n",
      "Iteration 760\tLL per word: -14.77604257434886\n",
      "Iteration 780\tLL per word: -14.776153290545555\n",
      "Iteration 800\tLL per word: -14.768231999713123\n",
      "Iteration 820\tLL per word: -14.770911036199522\n",
      "Iteration 840\tLL per word: -14.77328154294104\n",
      "Iteration 860\tLL per word: -14.766874011870764\n",
      "Iteration 880\tLL per word: -14.765442783795761\n",
      "Iteration 900\tLL per word: -14.771075077044534\n",
      "Iteration 920\tLL per word: -14.760721207347812\n",
      "Iteration 940\tLL per word: -14.765487493685555\n",
      "Iteration 960\tLL per word: -14.766637165365347\n",
      "Iteration 980\tLL per word: -14.766872026542156\n",
      "Topic #0\t확진, 감염, 환자, 코로나, 자\n",
      "Topic #1\t집회, 네이버, 아베, 매물, 검찰\n",
      "Topic #2\t지원, 지급, 지원금, 지사, 재난\n",
      "Topic #3\t태풍, 선, 하이, 기상청, 시\n",
      "Topic #4\t펀드, 투자, 뉴딜, 만, 투표\n",
      "Topic #5\t시장, 대학, 차, 교수, 기업\n",
      "Topic #6\t중국, 채용, 미국, 면접, 추천\n",
      "Topic #7\t장관, 추, 휴가, 김, 아들\n",
      "Topic #8\t의료, 트럼프, 대통령, 의사, 전공의\n",
      "Topic #9\t의원, 여성, 경찰, 혐의, 대표\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "import re # 정규표현식 패키지\n",
    "from kiwipiepy import Kiwi, Option\n",
    "import tomotopy as tp\n",
    "\n",
    "filename=\"./datas.txt\"  #파일 경로\n",
    "kiwi=Kiwi()\n",
    "kiwi.prepare()\n",
    "stopwords = set([\"명\", \"어제\", \"등\", \"이\",\"하\",\"돠\",\"있\",\"일\",\"것\",\"들\",\"적\",\"그\",\"씨\",\"되\",\"없\",\"다\",\"이다\"])\n",
    "\n",
    "\n",
    "def tokenize(sent): # 파일의 라인을 분석할 tokenize 함수\n",
    "    res, score = kiwi.analyze(sent)[0] # 첫번째 결과를 사용한다, 분석할때 나오는 결과에서 단어만 추출\n",
    "    return [word\n",
    "            for word, tag, _, _ in res\n",
    "            if tag.startswith('N') and word not in stopwords] #불용어사전 적용\n",
    "\n",
    "#N으로 시작하는 NNG (일반명사), NNP(고유명사), NNB(의존명사) NR(수사), NP(대명사)가 있다\n",
    "\n",
    "#pat_tag = re.compile('NN[GP]')\n",
    "#def tokenizer(raw, user_data):\n",
    "#        res, _ = user_data()[0]\n",
    "#        for w, tag, start, l in res:\n",
    "#            if not pat_tag.match(tag) or len(w) <= 1: continue\n",
    "#            yield w + ('다' if tag.startswith('V') else ''), start, l\n",
    "\n",
    "#위의 코드와 같이 정규표현식을 통해 원하는 형태로 추출 가능 일단은 ,명사위주로 해놨다.\n",
    "            \n",
    "\n",
    "model = tp.LDAModel(k=10, alpha=0.1, eta=0.01, min_cf=20,min_df=10, tw=tp.TermWeight.PMI)\n",
    "#LDA 모델을 적용해서 토픽 추출, k는 topic 개수\n",
    "#alpha는 문헌-토픽 디리클레 분포의 하이퍼 파라미터\n",
    "#eta는 토픽-단어 디리클레 분포의 하이퍼 파라미터 두개다 상수인듯하다\n",
    "#min_cf는 단어의 최소 장서 빈도수, 전체 문헌내 출현빈도\n",
    "#min_df는 단어의 최서 문헌 빈도수, 출현한 문헌 숫자수 의미\n",
    "#tw는 용어 가중치 기법으로, ONE, IDF, PMI를 사용가능, ONE 보다는 PMI나 IDF 둘중 하나 사용 \n",
    "\n",
    "\n",
    "for i, line in enumerate(open(filename, encoding='utf-8')): #해당 경로의 파일을 받아와 한 라인씩 model에 추가\n",
    "    model.add_doc(tokenize(line)) #추출하고 모델안의 문헌을 넣는다. 즉, 학습과정에 쓰일 문헌을 생성\n",
    "    if i % 10 == 0: print('Document #{} has been loaded'.format(i))\n",
    " \n",
    "model.train(0) #학습 초기화\n",
    "\n",
    "print('Total docs:', len(model.docs)) #문헌들\n",
    "print('Total words:', model.num_words) #전체단어개수\n",
    "print('Vocab size:', model.num_vocabs) #전체 어휘개수\n",
    " \n",
    " \n",
    "for i in range(0,1000,20):\n",
    "    print('Iteration {}\\tLL per word: {}'.format(i, model.ll_per_word))\n",
    "    model.train(20) #문헌 학습, 안에 숫자는 깁스 샘플링의 반복횟수\n",
    "\t\t#이때 기본값으로 시스템내 가용한 모든 스레드의 개수사용, 그리고 병렬화 방법을 찾아서 실행시켜준다\n",
    "  \n",
    "for i in range(model.k): #k는 토픽의 개수\n",
    "    res = model.get_topic_words(i, top_n=5) #하위 토픽 i에 해당하는 top_n개의 단어 반환\n",
    "    print('Topic #{}'.format(i), end='\\t')\n",
    "    print(', '.join(w for w, p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'nlptesting/document/datas.txt'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-08c6a34839ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# 이미 전처리된 코퍼스가 있으면 불러온다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'k.cps'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tomotopy\\utils.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'k.cps'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-08c6a34839ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     )\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# 입력 파일에는 한 줄에 문헌 하나씩이 들어가 있습니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkiwi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_analyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nlptesting/document/datas.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;31m# 전처리한 코퍼스를 저장한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'k.cps'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nlptesting/document/datas.txt'"
     ]
    }
   ],
   "source": [
    "import re # 정규표현식 패키지\n",
    "import tomotopy as tp # 토픽 모델링에 사용할 패키지\n",
    "from kiwipiepy import Kiwi # 한국어 형태소 분석에 사용할 패키지\n",
    "from pyvis.network import Network # 네트워크 시각화에 사용할 패키지\n",
    " \n",
    "try:\n",
    "    # 이미 전처리된 코퍼스가 있으면 불러온다.\n",
    "    corpus = tp.utils.Corpus.load('k.cps')\n",
    "except IOError:\n",
    "    # 없으면 전처리를 시작한다.\n",
    "    kiwi = Kiwi()\n",
    "    kiwi.prepare()\n",
    " \n",
    "    # 형태소 분석 후 사용할 태그, 원하는대로 넣을 수 있다.\n",
    "    \"\"\"pat_tag = re.compile('NN[GP]')\"\"\"\n",
    "    pat_tag = re.compile('NN[GP]|V[VA]|MAG|MM')\n",
    " \n",
    "    def tokenizer(raw, user_data):\n",
    "        res, _ = user_data()[0]\n",
    "        for w, tag, start, l in res:\n",
    "            if not pat_tag.match(tag) or len(w) <= 1: continue\n",
    "            yield w + ('다' if tag.startswith('V') else ''), start, l\n",
    " \n",
    "    corpus = tp.utils.Corpus(\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    # 입력 파일에는 한 줄에 문헌 하나씩이 들어가 있습니다.\n",
    "    corpus.process((line, kiwi.async_analyze(line)) for line in open('nlptesting/document/datas.txt', 'r',-1,'utf-8'))\n",
    "    # 전처리한 코퍼스를 저장한다.\n",
    "    corpus.save('k.cps')\n",
    " \n",
    "# 최소 10개 이상 문헌에 등장하고, 전체 출현빈도는 20 이상인 단어만 사용합니다.\n",
    "# 그리고 상위 10개 고빈도 단어는 분석에서 제외하구요\n",
    "# 주제 개수는 40개입니다.\n",
    "mdl = tp.CTModel(tw=tp.TermWeight.ONE, min_df=10, min_cf=20, k=10, corpus=corpus)\n",
    "mdl.train(0)\n",
    " \n",
    "# 문헌 수가 만 개 이상이라면 num_beta_sample을 1~5 정도로 줄여도 됨\n",
    "# 수 천개라면 최소 10 정도, 수 백개에 불과하다면 20 이상으로 키우는걸 권장\n",
    "mdl.num_beta_sample = 1\n",
    "print('Num docs:{}, Num Vocabs:{}, Total Words:{}'.format(\n",
    "    len(mdl.docs), len(mdl.used_vocabs), mdl.num_words\n",
    "))\n",
    "print('Removed Top words: ', *mdl.removed_top_words)\n",
    " \n",
    "# 총 1000회 깁스샘플링을 반복합니다.\n",
    "# 적절한 반복회수는 데이터에 따라 다릅니다.\n",
    "# ll_per_word 값의 증가가 둔화되거나 멈추는 지점까지만\n",
    "# 반복하는걸 추천합니다.\n",
    "for i in range(0, 1000, 20):\n",
    "    print('Iteration: {:04}, LL per word: {:.4}'.format(i, mdl.ll_per_word))\n",
    "    mdl.train(20)\n",
    "print('Iteration: {:04}, LL per word: {:.4}'.format(i, mdl.ll_per_word))\n",
    "\n",
    "for i in range(mdl.k):\n",
    "    res = mdl.get_topic_words(i, top_n=5)\n",
    "    print('Topic #{}'.format(i), end='\\t')\n",
    "    print(', '.join(w for w, p in res))\n",
    "\n",
    " # 학습된 결과를 시각화 합니다.\n",
    "g = Network(width=800, height=800, font_color=\"#333\")\n",
    "correl = mdl.get_correlations().reshape([-1])\n",
    "correl.sort()\n",
    " \n",
    "# 상관계수 상위 10%만 간선으로 잇습니다.\n",
    "top_tenth = mdl.k * (mdl.k - 1) // 10\n",
    "top_tenth = correl[-mdl.k - top_tenth]\n",
    " \n",
    "topic_counts = mdl.get_count_by_topics()\n",
    " \n",
    "for k in range(mdl.k):\n",
    "    label = \"#{}\".format(k)\n",
    "    title= ' '.join(word for word, _ in mdl.get_topic_words(k, top_n=8))\n",
    "    print('Topic', label, title)\n",
    "    label += '\\n' + ' '.join(word for word, _ in mdl.get_topic_words(k, top_n=3))\n",
    "    g.add_node(k, label=label, title=title, shape='ellipse', value=float(topic_counts[k]))\n",
    "    for l, correlation in zip(range(k - 1), mdl.get_correlations(k)):\n",
    "        if correlation < top_tenth: continue\n",
    "        g.add_edge(k, l, value=float(correlation), title='{:.02}'.format(correlation))\n",
    " \n",
    "g.barnes_hut(gravity=-1000, spring_length=20)\n",
    "g.show_buttons()\n",
    "# 시각화 파일이 topic_network.html 이라는 이름으로 저장됩니다.\n",
    "# 웹 브라우저로 열어서 확인해보세요.\n",
    "g.show(\"topic_network.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
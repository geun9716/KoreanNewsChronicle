{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0        美대표 “이행 점검 장·차관급 회담 年 6회 中, 합의 안 지킬 땐 관세 폭탄 즉시...\n",
      "1        법원 “여성 2명·자녀 6명 송환 의무 없어” 벨기에 “10세 이하 어린이는 데려올...\n",
      "2        “국제 원조 받아들이고 대선 다시 치러야”“美 인도주의적 쇼… 원조물품 일단 회수를...\n",
      "3        전과자 등 총기 소유 막기 위해 의무화 상원 반대·트럼프 거부에 통과 불투명민주당이...\n",
      "4        성능 실험서 배수 등 중대 결함 발견 새 장벽으로 교체 위해 8개 모두 철거미국과 ...\n",
      "                               ...                        \n",
      "26493    청문회서 결정적 흠결 드러나 / 잇단 부실 검증 책임 물어야 / 인사기준 강화가 우...\n",
      "26494    자유한국당이 축구장 선거유세 논란에 대해 “선거관리위원회로부터 유권해석을 받은 것”...\n",
      "26495    환경부가 대형마트, 백화점 등에서 1일부터 금지된 일회용 비닐봉투 사용에 대한 단속...\n",
      "26496    현대모비스는 일산 킨텍스에서 열린 ‘2019 서울모터쇼’를 통해 연말까지 글로벌 최...\n",
      "26497    문재인 대통령이 31일 조동호 과학기술정보통신부 장관 후보자에 대한 지명을 철회했다...\n",
      "Name: text, Length: 26498, dtype: object\n",
      "['것', '수', '이', '나', '사람', '등', '우리', '때', '년', '말', '일', '때문', '그것', '일', '문제', '사회', '중', '씨', '지금', '속', '하나', '집', '월', '데', '자신', '내', '경우', '명', '생각', '시간', '그녀', '앞', '번', '여자', '개', '전', '사실', '점', '정도', '원', '소리']\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'writer'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3d7155eae3c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'topics'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#해당 경로의 파일을 받아와 한 라인씩 model에 추가\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5138\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'writer'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "import io\n",
    "import re # 정규표현식 패키지\n",
    "from kiwipiepy import Kiwi, Option\n",
    "import tomotopy as tp\n",
    "import pandas as pd\n",
    "\n",
    "origin_path = 'C:/Users/geun/KoreanNewsChronicle/Data/201903.csv'\n",
    "result_path = \"C:/Users/geun/KoreanNewsChronicle/PreProcess/201903_topic.csv\"\n",
    "\n",
    "origin=pd.read_csv(origin_path,encoding='utf-8')\n",
    "\n",
    "text=origin['text']\n",
    "print(text)\n",
    "\n",
    "with open('C:/Users/geun/KoreanNewsChronicle/PreProcess/datas.txt','w',encoding='utf-8') as file:\n",
    "    for item in text:\n",
    "        file.write(\"%s\\n\" % item)\n",
    "\n",
    "class Reader:\n",
    "    def __init__(self, filePath):\n",
    "        self.file = open(filePath,'r',encoding='UTF-8')\n",
    "    \n",
    "    def read(self, id):\n",
    "        if id == 0: self.file.seek(0)\n",
    "        return self.file.readline()\n",
    "\n",
    "reader = Reader('datas.txt')\n",
    "\n",
    "file = open(\"한국어불용어100.txt\", 'r', encoding=\"utf-8\")\n",
    "stopword=[]\n",
    "\n",
    "while True:\n",
    "    line = file.readline()\n",
    "    if not line: break\n",
    "    wordlist = line.split('\\t')\n",
    "    if (wordlist[1].startswith('N')):\n",
    "        stopword.append(wordlist[0])\n",
    "\n",
    "print(stopword)\n",
    "filename=\"datas.txt\"  #파일 경로\n",
    "file = open(filename, 'r', encoding=\"UTF-8\")\n",
    "kiwi=Kiwi()\n",
    "kiwi.extract_add_words(reader.read)\n",
    "kiwi.prepare()\n",
    "stopwords = set(stopword)\n",
    "\n",
    "\n",
    "def tokenize(sent): # 파일의 라인을 분석할 tokenize 함수\n",
    "    res, score = kiwi.analyze(sent)[0] # 첫번째 결과를 사용한다, 분석할때 나오는 결과에서 단어만 추출\n",
    "    return [word\n",
    "            for word, tag, _, _ in res\n",
    "            if tag.startswith('N') and word not in stopwords] #불용어사전 적용\n",
    "\n",
    "\n",
    "#LDA 모델을 적용해서 토픽 추출, k는 topic 개수\n",
    "#alpha는 문헌-토픽 디리클레 분포의 하이퍼 파라미터\n",
    "#eta는 토픽-단어 디리클레 분포의 하이퍼 파라미터 두개다 상수인듯하다\n",
    "#min_cf는 단어의 최소 장서 빈도수, 전체 문헌내 출현빈도\n",
    "#min_df는 단어의 최서 문헌 빈도수, 출현한 문헌 숫자수 의미\n",
    "#tw는 용어 가중치 기법으로, ONE, IDF, PMI를 사용가능, ONE 보다는 PMI나 IDF 둘중 하나 사용 \n",
    "\n",
    "file = open(result_path, \"w\", encoding='utf-8', newline = '')\n",
    "writer = csv.writer(file)\n",
    "writer.writerow(['index', 'topics'])\n",
    "for i, line in enumerate(open(filename, encoding='utf-8')): #해당 경로의 파일을 받아와 한 라인씩 model에 추가\n",
    "    model = tp.LDAModel(k=1, alpha=0.1, eta=0.001, min_cf=3,min_df=1, tw=tp.TermWeight.PMI)\n",
    "    model.add_doc(tokenize(line)) #추출하고 모델안의 문헌을 넣는다. 즉, 학습과정에 쓰일 문헌을 생성\n",
    "    model.train(0) #학습 초기화\n",
    "    s = \"\"\n",
    "    for j in range(0,100):\n",
    "        model.train(20) #문헌 학습, 안에 숫자는 깁스 샘플링의 반복횟수\n",
    "\t\t#이때 기본값으로 시스템내 가용한 모든 스레드의 개수사용, 그리고 병렬화 방법을 찾아서 실행시켜준다\n",
    "    for k in range(model.k): #k는 토픽의 개수\n",
    "        res = model.get_topic_words(k, top_n=10) #하위 토픽 i에 해당하는 top_n개의 단어 반환\n",
    "        s = ', '.join(w for w, p in res)\n",
    "    writer.writerow([i, s])\n",
    "    # print(i, s)\n",
    "file.close()\n",
    "print(\"End\")\n",
    "\n",
    "topic = pd.read_csv(result_path, encoding='utf-8')\n",
    "\n",
    "# print(csv)\n",
    "# print(topic['topics'])\n",
    "\n",
    "merge = pd.concat([origin, topic['topics']], axis=1)\n",
    "print(merge)\n",
    "merge.to_csv('201903.csv', index=False, header=True, sep=\"\\t\")\n",
    "print('merge topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
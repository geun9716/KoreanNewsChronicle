{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "00c48d4780e7fa7f4b695bd71b2ac9722a525fe5d5880a11d633cd1f8478baa2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting MeCab\n",
      "  Downloading mecab-0.996.2-cp38-cp38-win_amd64.whl (510 kB)\n",
      "Installing collected packages: MeCab\n",
      "Successfully installed MeCab-0.996.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "WARNING: You are using pip version 20.1.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\zoo58\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchFiles(path):\n",
    "    filelist = []\n",
    "    filenames = os.listdir(path)\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(path, filename)\n",
    "        filelist.append(file_path)\n",
    "    return filelist\n",
    "    \n",
    "def getNVM_lemma(text):\n",
    "    tokenizer = MeCab.Tagger()\n",
    "    parsed = tokenizer.parse(text)\n",
    "    word_tag = [w for w in parsed.split(\"\\n\")]\n",
    "    pos = []\n",
    "    tags = ['NNG','NNP','VV','VA','VX','VCP','VCN', 'MAG']\n",
    "    for word_ in word_tag[:-2]:\n",
    "        word = word_.split(\"\\t\")\n",
    "        tag = word[1].split(\",\")\n",
    "        if(len(word[0]) < 2):\n",
    "            continue\n",
    "        if(tag[-1] != '*'):\n",
    "            t = tag[-1].split('/')\n",
    "            if(len(t[0])>1 and ('VV' in t[1] or 'VA' in t[1] or 'VX' in t[1])):\n",
    "                pos.append(t[0])\n",
    "        else:\n",
    "            if(tag[0] in tags):\n",
    "                pos.append(word[0])\n",
    "    return pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              date press category  \\\n0       2020/01/01  서울신문    국제,정치   \n1       2020/01/01  국민일보       정치   \n2       2020/01/01  국민일보       정치   \n3       2020/01/01  국민일보       경제   \n4       2020/01/01  서울신문       정치   \n...            ...   ...      ...   \n316384  2020/10/31  세계일보       정치   \n316385  2020/10/31  세계일보       정치   \n316386  2020/10/31  서울신문       국제   \n316387  2020/10/31  세계일보    사회,경제   \n316388  2020/10/31  세계일보       국제   \n\n                                             headline  \\\n0                            “교육이 계층이동 사다리 돼야 양극화 해소”   \n1                     국군 전사자 12만3000여명, 아직도 못 돌아오고 있다   \n2                 한국당의 처참한 성적표, “총사퇴는 쇼” “장기판 박카스 뚜껑”   \n3                  쌀값·외식비·학원비는 올랐는데, 소비자물가 상승률은 역대 최저   \n4                    “블루오션 잡아라”… 민주·정의, 청소년 당원 모시기 경쟁   \n...                                               ...   \n316384  서민 “조국, 자녀 관련 거짓말 끔찍이 싫어 고소… 아들 입대 두 달 남아” 압박   \n316385              ‘김신조 사건’ 후 52년… 북악산 철문 직접 연 文 대통령   \n316386          “주례 부탁드려도 될까요” 예비부부 부탁에 정세균 총리가 한 말은?   \n316387            서비스업 임금근로자 대출 늘고, 연체율도 최대… 올해가 더 걱정   \n316388     일일 신규 환자 10만명 넘었지만… 6대 경합주 격차 줄이는 ‘샤이 트럼프’   \n\n                                                      url  \\\n0       http://www.seoul.co.kr/news/newsView.php?id=20...   \n1       http://news.kmib.co.kr/article/view.asp?arcid=...   \n2       http://news.kmib.co.kr/article/view.asp?arcid=...   \n3       http://news.kmib.co.kr/article/view.asp?arcid=...   \n4       http://www.seoul.co.kr/news/newsView.php?id=20...   \n...                                                   ...   \n316384  http://www.segye.com/content/html/2020/10/31/2...   \n316385  http://www.segye.com/content/html/2020/10/31/2...   \n316386  http://www.seoul.co.kr/news/newsView.php?id=20...   \n316387  http://www.segye.com/content/html/2020/10/31/2...   \n316388  http://www.segye.com/content/html/2020/10/31/2...   \n\n                                                     text  \n0       [신년 인터뷰] ‘사회적 불평등’ 연구 대가 로버트 라이시 美 캘리포니아대학 교수 ...  \n1       사진=게티이미지6·25전쟁이 발발한 지 70년이 되는 새해에는 여섯 살 때 마지막으...  \n2       황교안(앞줄 왼쪽 네 번째) 자유한국당 대표가 31일 국회에서 인재영입위원들에게 임...  \n3       2019년 소비자물가 상승률이 전년 대비 0.4%로 1965년 통계 집계 이래 역대...  \n4       민주, 16~18세이하 청소년 위원 모집…정의, 대규모 입당식 등 만 18세 챙기기...  \n...                                                   ...  \n316384  ‘조국흑서’의 공동 저자이며 문재인 정부에 비판적 발언을 해온 서민 단국대 의대 교...  \n316385  문재인 대통령이 북한군 청와대 기습 시도 사건인 ‘김신조 사건’ 이후 52년만에 북...  \n316386  2020 코리아세일페스타 ‘최고!’정세균 국무총리가 31일 서울시청 ‘서울 ON’ ...  \n316387  지난해 서비스업에 종사하는 임금근로자의 대출이 크게 늘고 연체율도 높은 것으로 나타...  \n316388  트럼프·바이든 코로나19 대응으로 격돌미국 대선을 나흘 앞둔 30일(현지시간) 신종...  \n\n[316389 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "for filePath in  searchFiles('../Data/'):\n",
    "    review = pd.read_csv(filePath, encoding = 'utf-8')\n",
    "    reviews.append(review)\n",
    "docs = pd.concat(reviews, ignore_index=True)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-9f7cc89632d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtf_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetNVM_lemma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdtm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-b37507e2b8e8>\u001b[0m in \u001b[0;36mgetNVM_lemma\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetNVM_lemma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mparsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mword_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparsed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\MeCab.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[0m_MeCab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTagger_swiginit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_Tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparseToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf_vect = CountVectorizer(tokenizer=getNVM_lemma, min_df=2)\n",
    "dtm = tf_vect.fit_transform(docs['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tf_vect' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-11fd4dddd8c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf_vect' is not defined"
     ]
    }
   ],
   "source": [
    "vocab = dict()\n",
    "for idx, word in enumerate(tf_vect.get_feature_names()):\n",
    "    vocab[word] = dtm.getcol(idx).sum()  \n",
    "words = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
    "\n",
    "max = 20\n",
    "plt.bar(range(max), [i[1] for i in words[:max]])\n",
    "plt.title('Frequency Top 20')\n",
    "plt.xlabel('단어')\n",
    "ax = plt.subplot()\n",
    "ax.set_xticks(range(max))\n",
    "ax.set_xticklabels([i[0] for i in words[:max]], rotation = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}